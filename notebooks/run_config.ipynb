{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results shape: (1, 4, 4604, 1001)\n",
      "Mini circuits shape: (4, 4604)\n",
      "Results min/max: 0.000000 / 16.879414\n",
      "Results non-zero count: 22762\n",
      "Mini circuits sum: 12\n",
      "Mini circuits non-zero count: 12\n",
      "Any neurons with activity > 0: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sparse\n",
    "import h5py\n",
    "\n",
    "# Load the results from the test run\n",
    "test_path = \"/data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=memory_efficient_test/ckpt/\"\n",
    "\n",
    "# Load results\n",
    "Rs = sparse.load_npz(test_path + \"DNg100_Stim_Prune_Rs.npz\").todense()\n",
    "mini_circuits = sparse.load_npz(test_path + \"DNg100_Stim_Prune_mini_circuits.npz\").todense()\n",
    "\n",
    "print(f\"Results shape: {Rs.shape}\")\n",
    "print(f\"Mini circuits shape: {mini_circuits.shape}\")\n",
    "print(f\"Results min/max: {Rs.min():.6f} / {Rs.max():.6f}\")\n",
    "print(f\"Results non-zero count: {np.count_nonzero(Rs)}\")\n",
    "print(f\"Mini circuits sum: {mini_circuits.sum()}\")\n",
    "print(f\"Mini circuits non-zero count: {np.count_nonzero(mini_circuits)}\")\n",
    "\n",
    "# Check if any neurons are actually active\n",
    "print(f\"Any neurons with activity > 0: {(Rs > 0).any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# from src.vnc import run_vnc_simulation\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from hydra import initialize, compose\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import jax\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "# Configure JAX for better performance\n",
    "jax.config.update(\"jax_enable_x64\", False)  # Use float32 for better GPU performance\n",
    "# jax.config.update(\"jax_platforms\", \"cuda\")  # Prefer GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Use GPU 0\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.95\"\n",
    "\n",
    "# Disable XLA optimizations that might cause timing issues\n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_triton_gemm_any=false'\n",
    "\n",
    "import sparse\n",
    "from natsort import natsorted\n",
    "import src.io_dict_to_hdf5 as ioh5\n",
    "from src.plot_utils import *\n",
    "from src.vnc_sim import *\n",
    "from src.path_utils import *\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.auto import tqdm\n",
    "# Ensure custom resolvers are registered\n",
    "# This is important when loading saved configs that contain custom interpolations\n",
    "from src.path_utils import register_custom_resolvers\n",
    "register_custom_resolvers()\n",
    "\n",
    "\n",
    "def print_pytree(pytree):\n",
    "   \"\"\"\n",
    "   path_filter: function that takes key_path tuple and returns True/False\n",
    "   \"\"\"\n",
    "   def process_leaf(key_path, leaf):\n",
    "      print(f\"Key Path: {key_path[0]}, Leaf: {leaf.shape}\")\n",
    "\n",
    "   return jax.tree.map_with_path(process_leaf, pytree)\n",
    "\n",
    "def print_dict_shapes(data, prefix=\"\", max_depth=10, current_depth=0):\n",
    "   \"\"\"\n",
    "   Recursively print shapes for arrays and values for other types in a dictionary.\n",
    "   \n",
    "   Args:\n",
    "      data: Dictionary or other data structure to inspect\n",
    "      prefix: String prefix for indentation\n",
    "      max_depth: Maximum recursion depth to prevent infinite loops\n",
    "      current_depth: Current recursion depth\n",
    "   \"\"\"\n",
    "   import jax.numpy as jnp\n",
    "   import numpy as np\n",
    "   \n",
    "   if current_depth > max_depth:\n",
    "      print(f\"{prefix}... (max depth reached)\")\n",
    "      return\n",
    "   \n",
    "   if isinstance(data, dict):\n",
    "      for key, value in data.items():\n",
    "         print(f\"{prefix}{key}:\")\n",
    "         print_dict_shapes(value, prefix + \"  \", max_depth, current_depth + 1)\n",
    "   \n",
    "   elif isinstance(data, (list, tuple)):\n",
    "      print(f\"{prefix}Type: {type(data).__name__}, Length: {len(data)}\")\n",
    "      if len(data) > 0:\n",
    "         print(f\"{prefix}First element:\")\n",
    "         print_dict_shapes(data[0], prefix + \"  \", max_depth, current_depth + 1)\n",
    "         if len(data) > 1:\n",
    "               print(f\"{prefix}... ({len(data)-1} more elements)\")\n",
    "   \n",
    "   elif hasattr(data, 'shape'):  # Arrays (JAX, NumPy, etc.)\n",
    "      print(f\"{prefix}Type: {type(data).__name__}, Shape: {data.shape}, Dtype: {data.dtype}\")\n",
    "   \n",
    "   elif isinstance(data, (int, float, bool, str)):\n",
    "      print(f\"{prefix}Type: {type(data).__name__}, Value: {data}\")\n",
    "   \n",
    "   elif hasattr(data, '__dict__'):  # Objects with attributes\n",
    "      print(f\"{prefix}Type: {type(data).__name__}\")\n",
    "      for attr_name in dir(data):\n",
    "         if not attr_name.startswith('_'):  # Skip private attributes\n",
    "               try:\n",
    "                  attr_value = getattr(data, attr_name)\n",
    "                  if not callable(attr_value):  # Skip methods\n",
    "                     print(f\"{prefix}  {attr_name}:\")\n",
    "                     print_dict_shapes(attr_value, prefix + \"    \", max_depth, current_depth + 1)\n",
    "               except:\n",
    "                  print(f\"{prefix}  {attr_name}: <unable to access>\")\n",
    "   \n",
    "   else:\n",
    "      print(f\"{prefix}Type: {type(data).__name__}, Value: {str(data)[:100]}{'...' if len(str(data)) > 100 else ''}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=Testing/logs/run_config.yaml\n",
      "1 /data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=memory_efficient_test/logs/run_config.yaml\n",
      "2 /data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=mini_circuit_fix_test/logs/run_config.yaml\n",
      "üìÅ Loading config from: /data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=mini_circuit_fix_test/logs/run_config.yaml\n",
      "‚úÖ Loaded experiment: DNg100_Stim_Prune\n",
      "‚úÖ Successfully converted all paths to Path objects and created directories\n"
     ]
    }
   ],
   "source": [
    "experiment = 'DNg100_Stim_Prune'\n",
    "version = 'debug'\n",
    "# base_dir = Path(f'/gscratch/portia/eabe/Pugliese_2025/{experiment}/{version}')\n",
    "base_dir = Path(f'/data/users/eabe/Pugliese_2025/{experiment}/{version}')\n",
    "run_cfg_list = natsorted(list(Path(base_dir).rglob('run_config.yaml')))\n",
    "for n, run_cfg in enumerate(run_cfg_list):\n",
    "    print(n, run_cfg)\n",
    "\n",
    "# ###### Load and update config with specified paths template ###### \n",
    "\n",
    "cfg_num = -1\n",
    "# print(f'\\nüìÅ Loading config {cfg_num}: {run_cfg_list[cfg_num]}')\n",
    "\n",
    "# NEW APPROACH: Load config and replace paths using glados.yaml template\n",
    "cfg = load_config_with_path_template(\n",
    "    config_path=run_cfg_list[cfg_num],\n",
    "    paths_template=\"glados\",    # Use glados.yaml for local paths\n",
    "    experiment=experiment,      # This will override if needed\n",
    "    version=version,           # Use debug version locally instead of hyak\n",
    ")\n",
    "\n",
    "print(f'‚úÖ Loaded experiment: {cfg.experiment.name}')\n",
    "\n",
    "# Convert string paths to Path objects and create directories\n",
    "try:\n",
    "    cfg.paths = convert_dict_to_path(cfg.paths)\n",
    "    print(\"‚úÖ Successfully converted all paths to Path objects and created directories\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Warning: Could not convert all paths: {e}\")\n",
    "    print(\"Proceeding with string paths and manual directory creation...\")\n",
    "    # If conversion fails, create the directories manually\n",
    "    for key, path_str in cfg.paths.items():\n",
    "        if key != 'user' and isinstance(path_str, str):\n",
    "            path_obj = Path(path_str)\n",
    "            path_obj.mkdir(parents=True, exist_ok=True)\n",
    "            cfg.paths[key] = path_obj\n",
    "\n",
    "checkpoint_dir = None\n",
    "if cfg.sim.enable_checkpointing and hasattr(cfg, 'paths'):\n",
    "    checkpoint_dir = Path(cfg.paths.ckpt_dir) / \"checkpoints\"\n",
    "# print(f\"\\nüéØ Final paths configuration:\")\n",
    "# for key, value in cfg.paths.items():\n",
    "#     print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network configuration...\n",
      "Loaded neuron parameters from /data/users/eabe/Pugliese_2025/DNg100_Stim_Prune/debug/run_id=mini_circuit_fix_test/ckpt/neuron_params.h5\n",
      "Key Path: .W, Leaf: (4604, 4604)\n",
      "Key Path: .tau, Leaf: (4, 4604)\n",
      "Key Path: .a, Leaf: (4, 4604)\n",
      "Key Path: .threshold, Leaf: (4, 4604)\n",
      "Key Path: .fr_cap, Leaf: (4, 4604)\n",
      "Key Path: .input_currents, Leaf: (1, 4, 4604)\n",
      "Key Path: .seeds, Leaf: (4, 2)\n",
      "Key Path: .exc_dn_idxs, Leaf: (933,)\n",
      "Key Path: .inh_dn_idxs, Leaf: (385,)\n",
      "Key Path: .exc_in_idxs, Leaf: (1484,)\n",
      "Key Path: .inh_in_idxs, Leaf: (1658,)\n",
      "Key Path: .mn_idxs, Leaf: (144,)\n",
      "Key Path: .W_mask, Leaf: (4, 4604, 4604)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuronParams(W=None, tau=None, a=None, threshold=None, fr_cap=None, input_currents=None, seeds=None, exc_dn_idxs=None, inh_dn_idxs=None, exc_in_idxs=None, inh_in_idxs=None, mn_idxs=None, W_mask=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading network configuration...\")\n",
    "W_table = load_wTable(cfg.experiment.dfPath)\n",
    "n_stim_configs = len(cfg.experiment.stimNeurons)\n",
    "# Prepare parameters\n",
    "neuron_params = prepare_neuron_params(cfg, W_table, cfg.paths.ckpt_dir / 'neuron_params.h5')\n",
    "# neuron_params = prepare_neuron_params(cfg, W_table)\n",
    "sim_params = prepare_sim_params(cfg, n_stim_configs, neuron_params.W.shape[0])\n",
    "mn_mask = jnp.isin(jnp.arange(sim_params.n_neurons), neuron_params.mn_idxs)\n",
    "sim_config = parse_simulation_config(cfg)\n",
    "\n",
    "print_pytree(neuron_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([], shape=(0,), dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.where(neuron_params.W_mask[0,0] & ~mn_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([], shape=(0,), dtype=int32),)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.where(neuron_params.W_mask[0,:,0] & ~mn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.experiment.keepOnly = [31, 132]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment.removeNeurons=10,12,10'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "override = 'experiment.removeNeurons=[10],[12],[10]'\n",
    "override = 'experiment.removeNeurons=10,12,10'\n",
    "\n",
    "f\"{override:s}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mask = neuron_params.W_mask\n",
    "mn_mask = jnp.isin(jnp.arange(sim_params.n_neurons), neuron_params.mn_idxs)\n",
    "keep_neurons = jnp.zeros(sim_params.n_neurons, dtype=jnp.bool)\n",
    "keep_neurons = keep_neurons.at[cfg.experiment.keepOnly].set(True)\n",
    "keep_neurons = jnp.where(keep_neurons | mn_mask, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/users/eabe/Pugliese_2025/DN_Screen/debug/run_id=Testing/ckpt/checkpoints/results_checkpoint_batch_1\n",
      "batch_index:\n",
      "  Type: int, Value: 1\n",
      "completed_batches:\n",
      "  Type: int, Value: 2\n",
      "n_result_batches:\n",
      "  Type: int, Value: 2\n",
      "neuron_params:\n",
      "  W:\n",
      "    Type: ndarray, Shape: (4604, 4604), Dtype: float32\n",
      "  W_mask:\n",
      "    Type: ndarray, Shape: (16, 4604, 4604), Dtype: bool\n",
      "  a:\n",
      "    Type: ndarray, Shape: (16, 4604), Dtype: float32\n",
      "  exc_dn_idxs:\n",
      "    Type: ndarray, Shape: (933,), Dtype: int32\n",
      "  exc_in_idxs:\n",
      "    Type: ndarray, Shape: (1484,), Dtype: int32\n",
      "  fr_cap:\n",
      "    Type: ndarray, Shape: (16, 4604), Dtype: float32\n",
      "  inh_dn_idxs:\n",
      "    Type: ndarray, Shape: (385,), Dtype: int32\n",
      "  inh_in_idxs:\n",
      "    Type: ndarray, Shape: (1658,), Dtype: int32\n",
      "  input_currents:\n",
      "    Type: ndarray, Shape: (933, 16, 4604), Dtype: float32\n",
      "  mn_idxs:\n",
      "    Type: ndarray, Shape: (144,), Dtype: int32\n",
      "  seeds:\n",
      "    Type: ndarray, Shape: (16, 2), Dtype: uint32\n",
      "  tau:\n",
      "    Type: ndarray, Shape: (16, 4604), Dtype: float32\n",
      "  threshold:\n",
      "    Type: ndarray, Shape: (16, 4604), Dtype: float32\n",
      "total_batches:\n",
      "  Type: int, Value: 467\n"
     ]
    }
   ],
   "source": [
    "latest_checkpoint, base_name = find_latest_checkpoint(checkpoint_dir)\n",
    "print(latest_checkpoint)\n",
    "checkpoint_h5_path = latest_checkpoint / f\"{base_name}.h5\"\n",
    "metadata_yaml_path = latest_checkpoint / f\"{base_name}.yaml\"\n",
    "checkpoint_dict = ioh5.load(checkpoint_h5_path, enable_jax=False)\n",
    "\n",
    "metadata = OmegaConf.load(metadata_yaml_path)\n",
    "\n",
    "# Convert relevant arrays to JAX arrays manually\n",
    "for key in ['batch_index', 'completed_batches', 'total_batches', 'n_result_batches']:\n",
    "    if key in checkpoint_dict:\n",
    "        checkpoint_dict[key] = int(checkpoint_dict[key])\n",
    "\n",
    "# Extract basic fields\n",
    "basic_fields = {\n",
    "    'batch_index': int(checkpoint_dict['batch_index']),\n",
    "    'completed_batches': int(checkpoint_dict['completed_batches']),\n",
    "    'total_batches': int(checkpoint_dict['total_batches']),\n",
    "    'n_result_batches': int(checkpoint_dict['n_result_batches']),\n",
    "}\n",
    "print_dict_shapes(checkpoint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparse.load_npz(checkpoint_dir / f\"results_checkpoint_batch_{checkpoint_dict['batch_index']}/batch_{checkpoint_dict['batch_index']}.npz\").todense().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sim_utils import compute_oscillation_score, neuron_oscillation_score\n",
    "\n",
    "results = sparse.load_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_Rs.npz\").todense().astype(np.float32)\n",
    "# results = sparse.load_npz('/data/users/eabe/Pugliese_2025/prune_test/hyak/run_id=27996612/ckpt/prune_test_mini_circuits.npz').todense().astype(np.float32)\n",
    "min_circuit = sparse.load_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_mini_circuits.npz\").todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4604)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_circuit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Motor Neurons')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "R = results[0,2]\n",
    "w_table = pd.read_csv(cfg.experiment.dfPath,index_col=0)\n",
    "# 10093 = bdn2, 10707 = exc 1, 11751 = exc 2, 13905 = inh 1\n",
    "nonmns_idxs = w_table.loc[(w_table[\"bodyId\"]==10093) | (w_table[\"bodyId\"]==10707) | (w_table[\"bodyId\"]==11751) | (w_table[\"bodyId\"]==13905)].index\n",
    "mn_idxs = jnp.asarray(w_table.loc[w_table[\"class\"]==\"motor neuron\"].index.values)\n",
    "\n",
    "fig, axs = plt.subplots(1,3, figsize=(18, 6))\n",
    "for i in nonmns_idxs:\n",
    "    axs[0].plot(R[i])\n",
    "axs[0].set_title(\"Non-Motor Neurons\")\n",
    "for i in mn_idxs:\n",
    "    axs[1].plot(R[i])\n",
    "axs[1].set_title(\"Motor Neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([  31,  277,  617, 1167], dtype=int32),\n",
       " Array([  31,  277,  617, 1167], dtype=int32)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_circuit = jnp.stack([((jnp.sum(results[:,n],axis=-1)>0) & ~mn_mask) for n in range(results.shape[1])],axis=1)\n",
    "min_circuit.shape\n",
    "[jnp.where(min_circuit[0,n] & ~mn_mask)[0] for n in range(min_circuit.shape[1])]\n",
    "# sparse.save_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_mini_circuits.npz\", sparse.COO.from_numpy(min_circuit))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4604, 4604)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(min_circuit[0, :, None] * min_circuit[0, None, :]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_mask_init = jnp.ones((cfg.experiment.n_replicates,min_circuit.shape[-1],min_circuit.shape[-1]), dtype=jnp.bool_)\n",
    "\n",
    "W_mask_new = (W_mask_init * min_circuit[0, :, None] * min_circuit[0, None, :]).astype(jnp.bool_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237831f5fc6e4ef99c26fda774396ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.sim_utils import compute_oscillation_score, neuron_oscillation_score\n",
    "clip_start = 250# int(sim_params.pulse_start / sim_params.dt) + 200\n",
    "def compute_osc_score_all(R, mn_mask, clip_start=250):\n",
    "    # Get active MN activity using JAX-compatible approach\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_mask = ((max_frs > 0) & mn_mask)\n",
    "\n",
    "    # Compute oscillation score\n",
    "    oscillation_score, _ = compute_oscillation_score(R[..., clip_start:], active_mask, prominence=0.05)\n",
    "    return oscillation_score\n",
    "\n",
    "\n",
    "# osc_score_all = osc_vmap(results[0], mn_mask, clip_start)\n",
    "osc_score_all = []\n",
    "for replicate in tqdm(range(results.shape[1])):\n",
    "    osc_score = compute_osc_score_all(results[0,replicate], mn_mask, clip_start=clip_start)\n",
    "    osc_score_all.append(osc_score)\n",
    "osc_score_all = jnp.array(osc_score_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_mini = jnp.array([len(jnp.where(min_circuit[n] & ~mn_mask)[0]) for n in range(min_circuit.shape[0])])\n",
    "# jnp.where(min_circuit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4604)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_circuit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([  31,  277,  617, 1167], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.where(min_circuit[0] & ~mn_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([  31,  277,  617, 1167], dtype=int32),\n",
       " Array([  31,  277,  617, 1167], dtype=int32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[jnp.where(min_circuit[n] & ~mn_mask)[0] for n in range(min_circuit.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([  31,   86,  277,  617, 1167], dtype=int32),\n",
       " Array([1, 1, 3, 4, 3], dtype=int32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.unique(jnp.concatenate([jnp.where(min_circuit[n] & ~mn_mask)[0] for n in range(min_circuit.shape[0])],axis=0),return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bodyId                            12026\n",
       "type                           IN13A010\n",
       "class                  intrinsic neuron\n",
       "subclass                             IR\n",
       "hemilineage                         13A\n",
       "size                         2555798089\n",
       "predictedNt                        gaba\n",
       "predictedNtProb                0.659066\n",
       "ntAcetylcholineProb            0.006195\n",
       "ntGabaProb                     0.664261\n",
       "ntGlutamateProb                0.321424\n",
       "somaSide                            LHS\n",
       "motor module                        NaN\n",
       "step contribution                   NaN\n",
       "Name: 696, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_table.loc[696]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([], shape=(0,), dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.where(osc_score_all<0.5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7ff7fae10f10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(osc_score_all, n_neurons_mini )\n",
    "plt.axvline(x=0.5,c='k',ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024, 4604, 1001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m n = \u001b[32m10\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m plt.plot(results[n,min_circuit[\u001b[32m0\u001b[39m,n]].T)\n",
      "\u001b[31mIndexError\u001b[39m: index 10 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "plt.plot(results[0,n,min_circuit[0,n]].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49949652\n",
      "0.499279\n",
      "0.16256762\n",
      "0.23966448\n",
      "0.49967083\n",
      "0.0\n",
      "0.49933738\n",
      "0.21912007\n",
      "0.43483755\n",
      "0.49824634\n",
      "0.49943382\n",
      "0.208418\n",
      "0.49843386\n",
      "0.49899223\n",
      "0.0\n",
      "0.49852532\n",
      "0.48537502\n",
      "0.4008642\n",
      "0.49889615\n",
      "0.4985739\n",
      "0.49905136\n",
      "0.4520758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3512079/2413706698.py:10: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, axs = plt.subplots(1,3, figsize=(18, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39297423\n",
      "0.49921238\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.10841509\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.3931809\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4753139\n",
      "0.31009588\n",
      "0.0\n",
      "0.45856804\n",
      "0.4985162\n",
      "0.49649143\n",
      "0.49801844\n",
      "0.45881352\n",
      "0.4981954\n",
      "0.4981259\n",
      "0.49949148\n",
      "0.49832037\n",
      "0.0\n",
      "0.3834647\n",
      "0.0\n",
      "0.41936263\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.3028941\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.12709789\n",
      "0.121000275\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.04016412\n",
      "0.0\n",
      "0.0\n",
      "0.22171937\n",
      "0.2765023\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0300802\n",
      "0.16260664\n",
      "0.0\n",
      "0.30829886\n",
      "0.0\n",
      "0.0\n",
      "0.056725375\n",
      "0.11421431\n",
      "0.24252735\n",
      "0.0\n",
      "0.0\n",
      "0.4726712\n",
      "0.0\n",
      "0.0\n",
      "0.306064\n",
      "0.16582863\n",
      "0.0\n",
      "0.05823619\n",
      "0.33847177\n",
      "0.0\n",
      "0.06730034\n",
      "0.029685313\n",
      "0.20184675\n",
      "0.0\n",
      "0.0\n",
      "0.16382498\n",
      "0.075210854\n",
      "0.061827753\n",
      "0.056185097\n",
      "0.0\n",
      "0.0\n",
      "0.23180923\n",
      "0.0\n",
      "0.16160214\n",
      "0.0\n",
      "0.0\n",
      "0.057285942\n",
      "0.18083012\n",
      "0.05400975\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.06538169\n",
      "0.0\n",
      "0.05839609\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.41337052\n",
      "0.45746967\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.30966038\n",
      "0.0\n",
      "0.0\n",
      "0.43907145\n",
      "0.0\n",
      "0.49782616\n",
      "0.4992728\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.49871832\n",
      "0.0\n",
      "0.49817997\n",
      "0.4978408\n",
      "0.49915475\n",
      "0.4950146\n",
      "0.49928063\n",
      "0.4987231\n",
      "0.29332855\n",
      "0.4981515\n",
      "0.49939552\n",
      "0.49937668\n",
      "0.0\n",
      "0.49913603\n",
      "0.0\n",
      "0.49906772\n",
      "0.29995817\n",
      "0.4998565\n",
      "0.49874097\n",
      "0.4996765\n",
      "0.0\n",
      "0.35655096\n",
      "0.0\n",
      "0.4995645\n",
      "0.49916756\n",
      "0.0\n",
      "0.4410139\n",
      "0.4993802\n",
      "0.0\n",
      "0.49949944\n",
      "0.4977489\n",
      "0.0\n",
      "0.49899346\n",
      "0.49822748\n",
      "0.0\n",
      "0.0\n",
      "0.4996085\n",
      "0.4998069\n",
      "0.0\n",
      "0.0\n",
      "0.49938118\n",
      "0.4990574\n",
      "0.4997663\n",
      "0.0\n",
      "0.0\n",
      "0.16393974\n",
      "0.4993647\n",
      "0.49975738\n",
      "0.0\n",
      "0.0\n",
      "0.49978805\n",
      "0.0\n",
      "0.48588502\n",
      "0.49849665\n",
      "0.49863502\n",
      "0.4995907\n",
      "0.499107\n",
      "0.4992439\n",
      "0.49899685\n",
      "0.0\n",
      "0.49704027\n",
      "0.49877807\n",
      "0.0\n",
      "0.49819162\n",
      "0.23193423\n",
      "0.4025112\n",
      "0.48909548\n",
      "0.49844518\n",
      "0.4934931\n",
      "0.49874833\n",
      "0.49880227\n",
      "0.3048637\n",
      "0.49972707\n",
      "0.49925616\n",
      "0.1161173\n",
      "0.17843954\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m nonmns_idxs = w_table.loc[(w_table[\u001b[33m\"\u001b[39m\u001b[33mbodyId\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[32m10093\u001b[39m) | (w_table[\u001b[33m\"\u001b[39m\u001b[33mbodyId\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[32m10707\u001b[39m) | (w_table[\u001b[33m\"\u001b[39m\u001b[33mbodyId\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[32m11751\u001b[39m) | (w_table[\u001b[33m\"\u001b[39m\u001b[33mbodyId\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[32m13905\u001b[39m)].index\n\u001b[32m      8\u001b[39m mn_idxs = jnp.asarray(w_table.loc[w_table[\u001b[33m\"\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mmotor neuron\u001b[39m\u001b[33m\"\u001b[39m].index.values)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m fig, axs = plt.subplots(\u001b[32m1\u001b[39m,\u001b[32m3\u001b[39m, figsize=(\u001b[32m18\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nonmns_idxs:\n\u001b[32m     12\u001b[39m     axs[\u001b[32m0\u001b[39m].plot(R[i])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/pyplot.py:1776\u001b[39m, in \u001b[36msubplots\u001b[39m\u001b[34m(nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw, **fig_kw)\u001b[39m\n\u001b[32m   1631\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1632\u001b[39m \u001b[33;03mCreate a figure and a set of subplots.\u001b[39;00m\n\u001b[32m   1633\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1773\u001b[39m \n\u001b[32m   1774\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1775\u001b[39m fig = figure(**fig_kw)\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m axs = fig.subplots(nrows=nrows, ncols=ncols, sharex=sharex, sharey=sharey,\n\u001b[32m   1777\u001b[39m                    squeeze=squeeze, subplot_kw=subplot_kw,\n\u001b[32m   1778\u001b[39m                    gridspec_kw=gridspec_kw, height_ratios=height_ratios,\n\u001b[32m   1779\u001b[39m                    width_ratios=width_ratios)\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fig, axs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/figure.py:919\u001b[39m, in \u001b[36mFigureBase.subplots\u001b[39m\u001b[34m(self, nrows, ncols, sharex, sharey, squeeze, width_ratios, height_ratios, subplot_kw, gridspec_kw)\u001b[39m\n\u001b[32m    916\u001b[39m     gridspec_kw[\u001b[33m'\u001b[39m\u001b[33mwidth_ratios\u001b[39m\u001b[33m'\u001b[39m] = width_ratios\n\u001b[32m    918\u001b[39m gs = \u001b[38;5;28mself\u001b[39m.add_gridspec(nrows, ncols, figure=\u001b[38;5;28mself\u001b[39m, **gridspec_kw)\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m axs = gs.subplots(sharex=sharex, sharey=sharey, squeeze=squeeze,\n\u001b[32m    920\u001b[39m                   subplot_kw=subplot_kw)\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m axs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/gridspec.py:283\u001b[39m, in \u001b[36mGridSpecBase.subplots\u001b[39m\u001b[34m(self, sharex, sharey, squeeze, subplot_kw)\u001b[39m\n\u001b[32m    281\u001b[39m         subplot_kw[\u001b[33m\"\u001b[39m\u001b[33msharex\u001b[39m\u001b[33m\"\u001b[39m] = shared_with[sharex]\n\u001b[32m    282\u001b[39m         subplot_kw[\u001b[33m\"\u001b[39m\u001b[33msharey\u001b[39m\u001b[33m\"\u001b[39m] = shared_with[sharey]\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m         axarr[row, col] = figure.add_subplot(\n\u001b[32m    284\u001b[39m             \u001b[38;5;28mself\u001b[39m[row, col], **subplot_kw)\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# turn off redundant tick labeling\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sharex \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/figure.py:768\u001b[39m, in \u001b[36mFigureBase.add_subplot\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    766\u001b[39m         args = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mstr\u001b[39m(args[\u001b[32m0\u001b[39m])))\n\u001b[32m    767\u001b[39m     projection_class, pkw = \u001b[38;5;28mself\u001b[39m._process_projection_requirements(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m768\u001b[39m     ax = projection_class(\u001b[38;5;28mself\u001b[39m, *args, **pkw)\n\u001b[32m    769\u001b[39m     key = (projection_class, pkw)\n\u001b[32m    770\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_axes_internal(ax, key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axes/_base.py:696\u001b[39m, in \u001b[36m_AxesBase.__init__\u001b[39m\u001b[34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, forward_navigation_events, *args, **kwargs)\u001b[39m\n\u001b[32m    693\u001b[39m \u001b[38;5;28mself\u001b[39m.set_axisbelow(mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33maxes.axisbelow\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    695\u001b[39m \u001b[38;5;28mself\u001b[39m._rasterization_zorder = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m.clear()\n\u001b[32m    698\u001b[39m \u001b[38;5;66;03m# funcs used to format x and y - fall back on major formatters\u001b[39;00m\n\u001b[32m    699\u001b[39m \u001b[38;5;28mself\u001b[39m.fmt_xdata = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axes/_base.py:1416\u001b[39m, in \u001b[36m_AxesBase.clear\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28mself\u001b[39m.cla()\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1416\u001b[39m     \u001b[38;5;28mself\u001b[39m.__clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axes/_base.py:1332\u001b[39m, in \u001b[36m_AxesBase.__clear\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28mself\u001b[39m.legend_ = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1330\u001b[39m \u001b[38;5;28mself\u001b[39m.containers = []\n\u001b[32m-> \u001b[39m\u001b[32m1332\u001b[39m \u001b[38;5;28mself\u001b[39m.grid(\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Disable grid on init to use rcParameter\u001b[39;00m\n\u001b[32m   1333\u001b[39m \u001b[38;5;28mself\u001b[39m.grid(\u001b[38;5;28mself\u001b[39m._gridOn, which=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33maxes.grid.which\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1334\u001b[39m           axis=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33maxes.grid.axis\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m   1335\u001b[39m props = font_manager.FontProperties(\n\u001b[32m   1336\u001b[39m     size=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33maxes.titlesize\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   1337\u001b[39m     weight=mpl.rcParams[\u001b[33m'\u001b[39m\u001b[33maxes.titleweight\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axes/_base.py:3312\u001b[39m, in \u001b[36m_AxesBase.grid\u001b[39m\u001b[34m(self, visible, which, axis, **kwargs)\u001b[39m\n\u001b[32m   3310\u001b[39m _api.check_in_list([\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m], axis=axis)\n\u001b[32m   3311\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mx\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m3312\u001b[39m     \u001b[38;5;28mself\u001b[39m.xaxis.grid(visible, which=which, **kwargs)\n\u001b[32m   3313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m   3314\u001b[39m     \u001b[38;5;28mself\u001b[39m.yaxis.grid(visible, which=which, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axis.py:1746\u001b[39m, in \u001b[36mAxis.grid\u001b[39m\u001b[34m(self, visible, which, **kwargs)\u001b[39m\n\u001b[32m   1743\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmajor\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m   1744\u001b[39m     gridkw[\u001b[33m'\u001b[39m\u001b[33mgridOn\u001b[39m\u001b[33m'\u001b[39m] = (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._major_tick_kw[\u001b[33m'\u001b[39m\u001b[33mgridOn\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m   1745\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m visible \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m visible)\n\u001b[32m-> \u001b[39m\u001b[32m1746\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_tick_params(which=\u001b[33m'\u001b[39m\u001b[33mmajor\u001b[39m\u001b[33m'\u001b[39m, **gridkw)\n\u001b[32m   1747\u001b[39m \u001b[38;5;28mself\u001b[39m.stale = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axis.py:986\u001b[39m, in \u001b[36mAxis.set_tick_params\u001b[39m\u001b[34m(self, which, reset, **kwargs)\u001b[39m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mmajor\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m    985\u001b[39m     \u001b[38;5;28mself\u001b[39m._major_tick_kw.update(kwtrans)\n\u001b[32m--> \u001b[39m\u001b[32m986\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tick \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.majorTicks:\n\u001b[32m    987\u001b[39m         tick._apply_params(**kwtrans)\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m which \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mminor\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axis.py:547\u001b[39m, in \u001b[36m_LazyTickList.__get__\u001b[39m\u001b[34m(self, instance, owner)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._major:\n\u001b[32m    546\u001b[39m     instance.majorTicks = []\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m     tick = instance._get_tick(major=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    548\u001b[39m     instance.majorTicks = [tick]\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m instance.majorTicks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axis.py:1603\u001b[39m, in \u001b[36mAxis._get_tick\u001b[39m\u001b[34m(self, major)\u001b[39m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1600\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe Axis subclass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must define \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1601\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_tick_class or reimplement _get_tick()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1602\u001b[39m tick_kw = \u001b[38;5;28mself\u001b[39m._major_tick_kw \u001b[38;5;28;01mif\u001b[39;00m major \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._minor_tick_kw\n\u001b[32m-> \u001b[39m\u001b[32m1603\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tick_class(\u001b[38;5;28mself\u001b[39m.axes, \u001b[32m0\u001b[39m, major=major, **tick_kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/axis.py:379\u001b[39m, in \u001b[36mXTick.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# the y loc is 3 points below the min of y axis\u001b[39;00m\n\u001b[32m    378\u001b[39m trans, va, ha = \u001b[38;5;28mself\u001b[39m._get_text1_transform()\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m \u001b[38;5;28mself\u001b[39m.label1.set(\n\u001b[32m    380\u001b[39m     x=\u001b[32m0\u001b[39m, y=\u001b[32m0\u001b[39m,\n\u001b[32m    381\u001b[39m     verticalalignment=va, horizontalalignment=ha, transform=trans,\n\u001b[32m    382\u001b[39m )\n\u001b[32m    383\u001b[39m trans, va, ha = \u001b[38;5;28mself\u001b[39m._get_text2_transform()\n\u001b[32m    384\u001b[39m \u001b[38;5;28mself\u001b[39m.label2.set(\n\u001b[32m    385\u001b[39m     x=\u001b[32m0\u001b[39m, y=\u001b[32m1\u001b[39m,\n\u001b[32m    386\u001b[39m     verticalalignment=va, horizontalalignment=ha, transform=trans,\n\u001b[32m    387\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/artist.py:146\u001b[39m, in \u001b[36mArtist.__init_subclass__.<locals>.<lambda>\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m.set, \u001b[33m'\u001b[39m\u001b[33m_autogenerated_signature\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# Don't overwrite cls.set if the subclass or one of its parents\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# has defined a set method set itself.\u001b[39;00m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# If there was no explicit definition, cls.set is inherited from\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# the hierarchy of auto-generated set methods, which hold the\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# flag _autogenerated_signature.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[38;5;28mcls\u001b[39m.set = \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28mself\u001b[39m, **kwargs: Artist.set(\u001b[38;5;28mself\u001b[39m, **kwargs)\n\u001b[32m    147\u001b[39m \u001b[38;5;28mcls\u001b[39m.set.\u001b[34m__name__\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mcls\u001b[39m.set.\u001b[34m__qualname__\u001b[39m = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.set\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/artist.py:1241\u001b[39m, in \u001b[36mArtist.set\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m   1238\u001b[39m     \u001b[38;5;66;03m# docstring and signature are auto-generated via\u001b[39;00m\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Artist._update_set_signature_and_docstring() at the end of the\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# module.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1241\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._internal_update(cbook.normalize_kwargs(kwargs, \u001b[38;5;28mself\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/vnc-closedloop/lib/python3.11/site-packages/matplotlib/cbook.py:1793\u001b[39m, in \u001b[36mnormalize_kwargs\u001b[39m\u001b[34m(kw, alias_mapping)\u001b[39m\n\u001b[32m   1790\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot both \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcanonical_to_seen[canonical]\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1791\u001b[39m                         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m, which are aliases of one another\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1792\u001b[39m     canonical_to_seen[canonical] = k\n\u001b[32m-> \u001b[39m\u001b[32m1793\u001b[39m     ret[canonical] = v\n\u001b[32m   1795\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "check_ind = jnp.where(osc_score_all<0.5)[0]\n",
    "# for n in range(len(check_ind)):\n",
    "for n in range(10):\n",
    "    R = results[0][check_ind[n]]\n",
    "    print(osc_score_all[check_ind[n]])\n",
    "    w_table = pd.read_csv(cfg.experiment.dfPath,index_col=0)\n",
    "    # 10093 = bdn2, 10707 = exc 1, 11751 = exc 2, 13905 = inh 1\n",
    "    nonmns_idxs = w_table.loc[(w_table[\"bodyId\"]==10093) | (w_table[\"bodyId\"]==10707) | (w_table[\"bodyId\"]==11751) | (w_table[\"bodyId\"]==13905)].index\n",
    "    mn_idxs = jnp.asarray(w_table.loc[w_table[\"class\"]==\"motor neuron\"].index.values)\n",
    "\n",
    "    fig, axs = plt.subplots(1,3, figsize=(18, 6))\n",
    "    for i in nonmns_idxs:\n",
    "        axs[0].plot(R[i])\n",
    "    axs[0].set_title(\"Non-Motor Neurons\")\n",
    "    for i in mn_idxs:\n",
    "        axs[1].plot(R[i])\n",
    "    axs[1].set_title(\"Motor Neurons\")\n",
    "\n",
    "\n",
    "# scores001 = []\n",
    "# for i in range(results.shape[1]):\n",
    "#     R = results[0][i]\n",
    "#     max_frs = jnp.max(R, axis=-1)\n",
    "#     active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "#     activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "#     score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "#     scores001.append(score)\n",
    "# scores001 = jnp.concatenate(scores001, axis=-1)\n",
    "# axs[2].hist(scores001, bins=50, density=True)\n",
    "# plt.savefig(\"/data/users/eabe/Pugliese_2025/BND2_Stim_Test/debug/run_id=Testing/sim.noise=True/figures/Example_R.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sim_utils import compute_oscillation_score, neuron_oscillation_score\n",
    "clip_start = 250\n",
    "oscillation_threshold = 0.2\n",
    "# Load data (this part stays on CPU)\n",
    "w_table = load_wTable(cfg.experiment.dfPath)\n",
    "n_stim_configs = len(cfg.experiment.stimNeurons)\n",
    "## Initialize parameters\n",
    "neuron_params = prepare_neuron_params(cfg, w_table)\n",
    "sim_params = prepare_sim_params(cfg, n_stim_configs, neuron_params.W.shape[0])\n",
    "all_neurons = w_table.index.to_numpy()\n",
    "n_sims = sim_params.n_param_sets * sim_params.n_stim_configs\n",
    "mn_idxs = jnp.asarray(w_table.loc[w_table[\"class\"]==\"motor neuron\"].index.values)\n",
    "in_idxs = jnp.setdiff1d(all_neurons, mn_idxs)\n",
    "# max_frs = jnp.max(results[0], axis=-1)\n",
    "# mn_mask = jnp.isin(jnp.arange(R.shape[0]), mn_idxs)\n",
    "# active_mask = ((max_frs>0) & mn_mask)\n",
    "prominence = 0.05\n",
    "# Compute oscillation score\n",
    "# oscillation_score, _ = compute_oscillation_score(R[..., clip_start:], active_mask, prominence=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape, active_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(cfg.sim.pulseStart/cfg.sim.dt) + 100\n",
    "\n",
    "scores001 = []\n",
    "for i in range(results001.shape[1]):\n",
    "    R = results001[0][i]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "    activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "    score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "    scores001.append(score)\n",
    "scores01 = []\n",
    "for i in range(results01.shape[1]):\n",
    "    R = results01[0][i]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "    activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "    score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "    scores01.append(score)\n",
    "scores1 = []\n",
    "for i in range(results1.shape[1]):\n",
    "    R = results1[0][i]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "    activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "    score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "    scores1.append(score)\n",
    "scores10 = []\n",
    "for i in range(results10.shape[1]):\n",
    "    R = results10[0][i]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "    activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "    score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "    scores10.append(score)\n",
    "    \n",
    "    \n",
    "    \n",
    "# score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "# print(f\"Score: {jnp.nanmean(score)}, Frequency: {jnp.nanmean(frequency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2,figsize=(12, 12))\n",
    "axs = axs.flatten()\n",
    "axs[0].hist(jnp.concatenate(scores001, axis=-1), bins=25)\n",
    "axs[0].set_xlabel(\"Oscillation Score\")\n",
    "axs[0].set_title(\"Noise=0.01\")\n",
    "axs[1].hist(jnp.concatenate(scores01, axis=-1), bins=25)\n",
    "axs[1].set_xlabel(\"Oscillation Score\")\n",
    "axs[1].set_title(\"Noise=0.1\")\n",
    "axs[2].hist(jnp.concatenate(scores1, axis=-1), bins=25)\n",
    "axs[2].set_xlabel(\"Oscillation Score\")\n",
    "axs[2].set_title(\"Noise=1\")\n",
    "axs[3].hist(jnp.concatenate(scores10, axis=-1), bins=25)\n",
    "axs[3].set_xlabel(\"Oscillation Score\")\n",
    "axs[3].set_title(\"Noise=10\")\n",
    "plt.savefig(\"/data/users/eabe/Pugliese_2025/BND2_Stim_Test/debug/run_id=noisy_inputs/figures/oscillation_scores.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, config = load_vnc_net(cfg)\n",
    "simulator = OptimizedSimulator(params, config)\n",
    "# W, W_table = load_connectivity(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_queue = simulator._create_work_queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.shuffle_utils import shuffle_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newkey, key = jax.random.split(work_queue[0]['seed'])\n",
    "idxs = params.inh_dn_idxs\n",
    "W = params.W\n",
    "W_shuff = shuffle_W(W, key, idxs, independent=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0]), array([1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.all(np.sum(results,axis=-1)==0,axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = results[0][0]\n",
    "\n",
    "\n",
    "wTable = pd.read_csv(\"../data/manc t1 connectome data/wTable_20231020_DNtoMN_unsorted_withModules.csv\",index_col=0)\n",
    "nonMns = wTable.loc[(wTable[\"bodyId\"]==10093) | (wTable[\"bodyId\"]==10707) | (wTable[\"bodyId\"]==13905) | (wTable[\"bodyId\"]==11751)]\n",
    "mnIdxs = wTable.loc[wTable[\"class\"]==\"motor neuron\"].index\n",
    "\n",
    "\n",
    "for i in nonMns.index:\n",
    "# for i in mnIdxs:\n",
    "    plt.plot(R[i])\n",
    "    #plt.plot(Rtsp[i])\n",
    "\n",
    "print(np.mean(R))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing osc score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparse.load_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_Rs.npz\").todense().astype(np.float32)\n",
    "\n",
    "wTable = pd.read_csv(\"../data/manc t1 connectome data/wTable_20231020_DNtoMN_unsorted_withModules.csv\",index_col=0)\n",
    "non_mns = (wTable.loc[(wTable[\"bodyId\"]==10093) | (wTable[\"bodyId\"]==10707) | (wTable[\"bodyId\"]==13905) | (wTable[\"bodyId\"]==11751)]).values\n",
    "mn_idxs = wTable.loc[wTable[\"class\"]==\"motor neuron\"].index.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "import numpy as np\n",
    "def neuron_oscillation_score_helper_old(activity,prominence):\n",
    "    activity = activity-np.min(activity)\n",
    "    activity = 2 * activity/np.max(activity) - 1\n",
    "\n",
    "    autocorr = np.correlate(activity,activity,mode=\"full\") / np.inner(activity,activity)\n",
    "    lags = signal.correlation_lags(len(activity),len(activity))\n",
    "    autocorr = autocorr[lags>0]\n",
    "    lags = lags[lags>0]\n",
    "\n",
    "    peaks, peakProperties = signal.find_peaks(autocorr,height=(None,None),prominence=prominence)\n",
    "    if len(peaks) > 0:\n",
    "        score = np.min([np.max(peakProperties[\"peak_heights\"]),np.max(peakProperties[\"prominences\"])])\n",
    "        frequency = 1 / peaks[np.argmax(peakProperties[\"prominences\"])]\n",
    "    else:\n",
    "        score = 0\n",
    "        frequency = 0\n",
    "\n",
    "    return score, frequency\n",
    "\n",
    "def neuron_oscillation_score_old(activity, returnFrequency=False,prominence=0.05):\n",
    "    rawScore, frequency = neuron_oscillation_score_helper_old(activity,prominence)\n",
    "    # normalize to sine wave of the same frequency and duration\n",
    "    if rawScore == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        refSinScore, _ = neuron_oscillation_score_helper_old(np.sin(2*np.pi*frequency*np.arange(len(activity))),prominence)\n",
    "        refCosScore, _ = neuron_oscillation_score_helper_old(np.cos(2*np.pi*frequency*np.arange(len(activity))),prominence)\n",
    "        refScore = np.max((refSinScore,refCosScore))\n",
    "        score = rawScore / refScore\n",
    "\n",
    "    if returnFrequency:\n",
    "        return score, frequency\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "def sim_oscillation_score_old(R,activeMnIdxs,start=None,end=None,returnFrequency=False):\n",
    "    \"\"\"calculate oscillation score for a simulation\"\"\"\n",
    "    if start is None:\n",
    "        start = 0\n",
    "    if end is None:\n",
    "        end = -1\n",
    "\n",
    "    if returnFrequency:\n",
    "        neuronOscillationScores = []\n",
    "        frequencies = []\n",
    "\n",
    "        for j in activeMnIdxs:\n",
    "            score, freq = neuron_oscillation_score_old(R[j][start:end],returnFrequency=True)\n",
    "            neuronOscillationScores.append(score)\n",
    "            frequencies.append(freq)\n",
    "        return np.mean(neuronOscillationScores), np.nanmean(frequencies)\n",
    "        \n",
    "    else:\n",
    "        neuronOscillationScores = [neuron_oscillation_score_old(R[j][start:end]) for j in activeMnIdxs] # scores for each neuron\n",
    "        return np.mean(neuronOscillationScores) # average for the simulation\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = results[0][0]\n",
    "np_R = np.asarray(R)\n",
    "maxFrs = np.max(np_R,axis=-1)\n",
    "activeMnIdxs = mn_idxs[maxFrs[mn_idxs]>0]\n",
    "plt.plot(np_R[activeMnIdxs].T)\n",
    "plt.show()\n",
    "start = int(cfg.sim.pulseStart/cfg.sim.dt) + 200\n",
    "print(f\"Start index: {start}\")\n",
    "score, freq = sim_oscillation_score_old(R,activeMnIdxs,start=start,end=None,returnFrequency=True)\n",
    "print(f\"Score: {score}, Frequency: {freq}\")\n",
    "\n",
    "##### New Method #####\n",
    "# R = results[0][0]\n",
    "\n",
    "max_frs = jnp.max(R, axis=-1)\n",
    "active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "\n",
    "score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "\n",
    "print(f\"Score: {jnp.nanmean(score)}, Frequency: {jnp.nanmean(frequency)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(cfg.sim.pulseStart/cfg.sim.dt) + 100\n",
    "R = results\n",
    "max_frs = jnp.max(R, axis=-1)\n",
    "mn_mask = jnp.isin(jnp.arange(R.shape[2]), mn_idxs)\n",
    "active_mask = ((max_frs > 0) & mn_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.signal import correlate\n",
    "from src.sim_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity = results[0][1][451, start:]  # Adjusted to match the original code's start index\n",
    "\n",
    "activity = activity - jnp.min(activity, axis=-1, keepdims=True)\n",
    "activity = 2 * activity / jnp.max(activity, axis=-1, keepdims=True) - 1\n",
    "\n",
    "autocorr = autocorrelation_1d(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorr_scipy = correlate(activity, activity, mode='full', method='fft')\n",
    "autocorr_scipy = autocorr_scipy/jnp.max(autocorr_scipy)\n",
    "autocorr_old = np.correlate(activity,activity,mode=\"full\") / np.inner(activity,activity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(autocorr, label='JAX Autocorrelation')\n",
    "plt.plot(autocorr_scipy, label='SciPy Autocorrelation')\n",
    "plt.plot(autocorr_old, label='Old NumPy Autocorrelation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.where(active_mask[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = int(cfg.sim.pulseStart/cfg.sim.dt) + 100\n",
    "R = results[0][0]\n",
    "\n",
    "max_frs = jnp.max(R, axis=-1)\n",
    "active_mn_idxs = mn_idxs[max_frs[...,mn_idxs] > 0]\n",
    "activity = R[active_mn_idxs][..., start:]  # Adjusted to match the original code's start index\n",
    "\n",
    "score, frequency = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(activity, 0.05)\n",
    "\n",
    "print(f\"Score: {jnp.nanmean(score)}, Frequency: {jnp.nanmean(frequency)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sim_utils import neuron_oscillation_score\n",
    "\n",
    "experiment='prune_test'\n",
    "sim = 'prune_network'\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    cfg=compose(config_name='config.yaml', overrides= [f\"experiment={experiment}\", f\"sim={sim}\", \"paths=glados\", \"version=debug\", f'run_id=Testing'],return_hydra_config=True,)\n",
    "    HydraConfig.instance().set_config(cfg)\n",
    "\n",
    "for k in cfg.paths.keys():\n",
    "    if (k != 'user'):\n",
    "        cfg.paths[k] = Path(cfg.paths[k])\n",
    "        cfg.paths[k].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def print_inds(arr, name=''):\n",
    "    print(f\"Indices for {name}: {jnp.where(arr)[0].shape}\")\n",
    "    return jnp.where(arr)[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## full pipeline testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_table = load_wTable(cfg.experiment.dfPath)\n",
    "\n",
    "n_stim_configs = len(cfg.experiment.stimNeurons)\n",
    "\n",
    "# Prepare parameters\n",
    "neuron_params = prepare_neuron_params(cfg, W_table)\n",
    "sim_params = prepare_sim_params(cfg, n_stim_configs, neuron_params.W.shape[0])\n",
    "sim_config = parse_simulation_config(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sims = sim_params.n_stim_configs * sim_params.n_param_sets\n",
    "n_devices = jax.device_count()\n",
    "\n",
    "# Calculate batch size\n",
    "batch_size = sim_config.batch_size\n",
    "if batch_size is None:\n",
    "    batch_size = calculate_optimal_batch_size(\n",
    "        sim_params.n_neurons, len(sim_params.t_axis), n_devices\n",
    "    )\n",
    "\n",
    "# Adjust batch size for multiple devices\n",
    "if n_devices > 1:\n",
    "    batch_size = (batch_size // n_devices) * n_devices\n",
    "    if batch_size == 0:\n",
    "        batch_size = n_devices\n",
    "\n",
    "# Get batch processing function\n",
    "batch_func = get_batch_function(sim_config)\n",
    "\n",
    "# Create parallel version for multiple devices\n",
    "if n_devices > 1:\n",
    "    batch_func = pmap(batch_func, axis_name=\"device\", in_axes=(None, None, 0))\n",
    "\n",
    "print(f\"Running {total_sims} {sim_config.sim_type} simulations with batch size {batch_size} on {n_devices} device(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-compute static values that won't change during the loop\n",
    "oscillation_threshold_val = float(sim_config.oscillation_threshold)\n",
    "clip_start_val = int(sim_params.pulse_start / sim_params.dt) + 200\n",
    "\n",
    "# Create a version of update_single_sim_state with static args baked in\n",
    "def update_state_with_static_args(state, R, mn_mask):\n",
    "    jax.debug.print(\"W_mask shape: {W_mask.shape}\", W_mask=state.W_mask)\n",
    "    return update_single_sim_state(state, R, mn_mask, oscillation_threshold_val, clip_start_val)\n",
    "\n",
    "# Apply JIT to the wrapper function (not the original)\n",
    "jitted_update = jax.jit(update_state_with_static_args)\n",
    "\n",
    "# Now vmap this wrapper with only the traced arguments\n",
    "batch_update = jax.vmap(\n",
    "    jitted_update, \n",
    "    in_axes=(0, 0, 0)  # state, R, mn_mask - all batched\n",
    ")\n",
    "\n",
    "# Create parallel versions for multiple devices\n",
    "if n_devices > 1:\n",
    "    batch_update = pmap(batch_update, axis_name=\"device\", in_axes=(0, 0, 0))\n",
    "\n",
    "# Rest of the function...\n",
    "all_results = []\n",
    "n_batches = (total_sims + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(n_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, total_sims)\n",
    "    actual_batch_size = end_idx - start_idx\n",
    "    \n",
    "    batch_indices = jnp.arange(start_idx, end_idx)\n",
    "    \n",
    "    # Pad batch_indices if necessary for pmap\n",
    "    if n_devices > 1 and len(batch_indices) < batch_size:\n",
    "        pad_size = batch_size - len(batch_indices)\n",
    "        batch_indices = jnp.concatenate([\n",
    "            batch_indices, \n",
    "            jnp.repeat(batch_indices[-1], pad_size)\n",
    "        ])\n",
    "    \n",
    "    mn_idxs = neuron_params.mn_idxs\n",
    "    \n",
    "    # Initialize state for this batch\n",
    "    current_batch_size = len(batch_indices)\n",
    "    state = initialize_pruning_state(neuron_params, sim_params, current_batch_size)\n",
    "\n",
    "    # Reshape state for pmap if using multiple devices\n",
    "    if n_devices > 1:\n",
    "        state = reshape_state_for_pmap(state, n_devices)\n",
    "        batch_indices = batch_indices.reshape(n_devices, -1)\n",
    "\n",
    "    # Main pruning loop\n",
    "    iteration = 0\n",
    "    \n",
    "    # Clear GPU memory before processing\n",
    "    jax.clear_caches()\n",
    "    \n",
    "    # while True:\n",
    "    # Check convergence condition\n",
    "    all_converged = jnp.all(state.min_circuit)\n",
    "    \n",
    "    if all_converged or (iteration >= sim_config.max_pruning_iterations):\n",
    "        break\n",
    "        \n",
    "    start_time = time.time()\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    \n",
    "    # Update neuron parameters W_mask based on the current state\n",
    "    if n_devices > 1:\n",
    "        flat_W_mask = reshape_state_from_pmap(state).W_mask\n",
    "        neuron_params = neuron_params._replace(W_mask=flat_W_mask)\n",
    "    else:\n",
    "        neuron_params = neuron_params._replace(W_mask=state.W_mask)\n",
    "\n",
    "    # Run simulation\n",
    "    batch_results = batch_func(neuron_params, sim_params, batch_indices)\n",
    "    \n",
    "    # Create mn_mask and broadcast to batch dimensions\n",
    "    mn_mask = jnp.isin(jnp.arange(sim_params.n_neurons), mn_idxs)\n",
    "    \n",
    "    if n_devices > 1:\n",
    "        # For pmap, broadcast to (n_devices, batch_per_device, n_neurons)\n",
    "        batch_per_device = batch_indices.shape[1]\n",
    "        mn_mask_batch = jnp.broadcast_to(mn_mask, (n_devices, batch_per_device, len(mn_mask)))\n",
    "    else:\n",
    "        # For single device, broadcast to (batch_size, n_neurons)\n",
    "        mn_mask_batch = jnp.broadcast_to(mn_mask, (batch_results.shape[0], len(mn_mask)))\n",
    "        \n",
    "    # Update state - now only passing traced arguments\n",
    "    print(\"Updating state with batch results\")\n",
    "    print(f\"W_mask shape: {state.W_mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_pytree(neuron_params)\n",
    "# sim_config\n",
    "# print_pytree(batch_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state.W_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oscillation_threshold = 0.2\n",
    "# Load data (this part stays on CPU)\n",
    "w_table = load_wTable(cfg.experiment.dfPath)\n",
    "n_stim_configs = len(cfg.experiment.stimNeurons)\n",
    "## Initialize parameters\n",
    "neuron_params = prepare_neuron_params(cfg, w_table)\n",
    "sim_params = prepare_sim_params(cfg, n_stim_configs, neuron_params.W.shape[0])\n",
    "all_neurons = w_table.index.to_numpy()\n",
    "n_sims = sim_params.n_param_sets * sim_params.n_stim_configs\n",
    "mn_idxs = jnp.asarray(w_table.loc[w_table[\"class\"]==\"motor neuron\"].index.values)\n",
    "in_idxs = jnp.setdiff1d(all_neurons, mn_idxs)\n",
    "clip_start = int(cfg.sim.pulseStart / cfg.sim.dt) + 100\n",
    "\n",
    "total_sims = sim_params.n_stim_configs * sim_params.n_param_sets\n",
    "batch_size = getattr(cfg.experiment, \"batch_size\", None)\n",
    "if batch_size is None:\n",
    "    batch_size = calculate_optimal_batch_size(\n",
    "        sim_params.n_neurons, len(sim_params.t_axis)\n",
    "    )\n",
    "    \n",
    "batch_func = process_batch_prune\n",
    "# Create parallel version for multiple devices\n",
    "if jax.device_count() > 1:\n",
    "    batch_func = pmap(batch_func, axis_name=\"device\", in_axes=(None, None, 0))\n",
    "    batch_size = (batch_size // jax.device_count()) * jax.device_count()\n",
    "\n",
    "print(f\"Running {total_sims} simulations with batch size {batch_size}\")\n",
    "\n",
    "# Process in batches\n",
    "all_results = []\n",
    "n_batches = (total_sims + batch_size - 1) // batch_size\n",
    "\n",
    "# for i in range(n_batches):\n",
    "i = 0 \n",
    "start_idx = i * batch_size\n",
    "end_idx = min((i + 1) * batch_size, total_sims)\n",
    "\n",
    "batch_indices = jnp.arange(start_idx, end_idx)\n",
    "\n",
    "# Pad if necessary for pmap\n",
    "if jax.device_count() > 1 and len(batch_indices) < batch_size:\n",
    "    pad_size = batch_size - len(batch_indices)\n",
    "    batch_indices = jnp.concatenate([\n",
    "        batch_indices, \n",
    "        jnp.repeat(batch_indices[-1], pad_size)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mn_idxs = neuron_params.mn_idxs\n",
    "all_neurons = jnp.arange(neuron_params.W.shape[-1])\n",
    "in_idxs = jnp.setdiff1d(all_neurons, mn_idxs)\n",
    "\n",
    "clip_start = int(sim_params.pulse_start / sim_params.dt) + 100\n",
    "# Initialize state\n",
    "n_sims = sim_params.n_param_sets * sim_params.n_stim_configs\n",
    "W_mask = jnp.full((n_sims, neuron_params.W.shape[0], neuron_params.W.shape[1]), 1, dtype=jnp.float32)\n",
    "interneuron_mask = jnp.full((n_sims, W_mask.shape[-1]),fill_value=False, dtype=jnp.bool_)\n",
    "interneuron_mask = interneuron_mask.at[:,in_idxs].set(True)\n",
    "level = jnp.zeros((n_sims,1), dtype=jnp.int32)\n",
    "total_removed_neurons = jnp.full((n_sims, W_mask.shape[-1]), False, dtype=jnp.bool)\n",
    "removed_stim_neurons = jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "neurons_put_back =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "prev_put_back =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "last_removed =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool) # Use False for None\n",
    "min_circuit = jnp.full((n_sims,1), False)\n",
    "\n",
    "# Initialize probabilities\n",
    "exclude_mask = (~interneuron_mask)\n",
    "p_arrays = jax.vmap(removal_probability)(jnp.ones(len(interneuron_mask)), exclude_mask)\n",
    "\n",
    "# initialize pruning state\n",
    "state = Pruning_state(\n",
    "    W_mask=W_mask,\n",
    "    interneuron_mask=interneuron_mask,\n",
    "    level=level,\n",
    "    total_removed_neurons=total_removed_neurons,\n",
    "    removed_stim_neurons=removed_stim_neurons,\n",
    "    neurons_put_back=neurons_put_back,\n",
    "    prev_put_back=prev_put_back,\n",
    "    last_removed=last_removed,\n",
    "    remove_p=p_arrays,\n",
    "    min_circuit=min_circuit,\n",
    "    keys=neuron_params.seeds\n",
    ")\n",
    "\n",
    "iter_start = 0  # Starting iteration\n",
    "# Main pruning loop\n",
    "iteration = iter_start\n",
    "max_iterations = 1  # Safety limit\n",
    "while not jnp.all(min_circuit) and (iteration < max_iterations):\n",
    "    start_time = time.time()\n",
    "    print(f\"Iteration {iteration}\")\n",
    "    # Update neuron parameters W_mask based on the current state\n",
    "    neuron_params = neuron_params._replace(W_mask=state.W_mask)\n",
    "    # Run simulation (this would call your simulation function)\n",
    "    # Reshape for devices if using pmap\n",
    "    if jax.device_count() > 1:\n",
    "        batch_indices = batch_indices.reshape(jax.device_count(), -1)\n",
    "        batch_results = batch_func(neuron_params, sim_params, batch_indices)\n",
    "        batch_results = batch_results.reshape(-1, *batch_results.shape[2:])\n",
    "        batch_results = batch_results[:end_idx - start_idx]  # Remove padding\n",
    "    else:\n",
    "        batch_results = batch_func(neuron_params, sim_params, batch_indices)\n",
    "    \n",
    "    n = 0 \n",
    "    state = jax.vmap(update_single_sim_state, in_axes=(0, 0, None, None, None))(state, batch_results, mn_idxs, oscillation_threshold, clip_start)\n",
    "    iteration += 1\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"  Total time: {elapsed:.2f} seconds\")\n",
    "\n",
    "# batch_results = jax.device_put(batch_results, jax.devices(\"cpu\")[0])\n",
    "# all_results.append(batch_results)\n",
    "# print(f\"Batch {i + 1}/{n_batches} completed\")\n",
    "\n",
    "# del batch_results  # Free memory\n",
    "# gc.collect()  # Force garbage collection\n",
    "\n",
    "# Combine results\n",
    "# results = jnp.concatenate(all_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mn_idxs = neuron_params.mn_idxs\n",
    "all_neurons = jnp.arange(neuron_params.W.shape[-1])\n",
    "in_idxs = jnp.setdiff1d(all_neurons, mn_idxs)\n",
    "\n",
    "clip_start = int(sim_params.pulse_start / sim_params.dt) + 100\n",
    "# Initialize state\n",
    "n_sims = sim_params.n_param_sets * sim_params.n_stim_configs\n",
    "W_mask = jnp.full((n_sims, neuron_params.W.shape[0], neuron_params.W.shape[1]), 1, dtype=jnp.float32)\n",
    "interneuron_mask = jnp.full((n_sims, W_mask.shape[-1]),fill_value=False, dtype=jnp.bool_)\n",
    "interneuron_mask = interneuron_mask.at[:,in_idxs].set(True)\n",
    "level = jnp.zeros((n_sims,1), dtype=jnp.int32)\n",
    "total_removed_neurons = jnp.full((n_sims, W_mask.shape[-1]), False, dtype=jnp.bool)\n",
    "removed_stim_neurons = jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "neurons_put_back =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "prev_put_back =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool)\n",
    "last_removed =  jnp.full([n_sims, total_removed_neurons.shape[-1]], False, dtype=jnp.bool) # Use False for None\n",
    "min_circuit = jnp.full((n_sims,1), False)\n",
    "\n",
    "# Initialize probabilities\n",
    "exclude_mask = (~interneuron_mask)\n",
    "p_arrays = jax.vmap(removal_probability)(jnp.ones(len(interneuron_mask)), exclude_mask)\n",
    "\n",
    "# initialize pruning state\n",
    "state = Pruning_state(\n",
    "    W_mask=W_mask[0],\n",
    "    interneuron_mask=interneuron_mask[0],\n",
    "    level=level[0],\n",
    "    total_removed_neurons=total_removed_neurons[0],\n",
    "    removed_stim_neurons=removed_stim_neurons[0],\n",
    "    neurons_put_back=neurons_put_back[0],\n",
    "    prev_put_back=prev_put_back[0],\n",
    "    last_removed=last_removed[0],\n",
    "    remove_p=p_arrays[0],\n",
    "    min_circuit=min_circuit[0],\n",
    "    keys=neuron_params.seeds[0],\n",
    ")\n",
    "R = batch_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Unpack state\n",
    "(W_mask, interneuron_mask, level, total_removed_neurons, removed_stim_neurons,\n",
    "    neurons_put_back, last_removed, remove_p, min_circuit, key) = state\n",
    "\n",
    "# Get active MN activity using JAX-compatible approach\n",
    "max_frs = jnp.max(R, axis=-1)\n",
    "mn_mask = jnp.isin(jnp.arange(R.shape[0]), mn_idxs)\n",
    "active_mask = ((max_frs>0) & mn_mask)\n",
    "\n",
    "# Compute oscillation score\n",
    "oscillation_score, _ = compute_oscillation_score(R[..., clip_start:], active_mask, prominence=0.05)\n",
    "\n",
    "# Check if oscillation is below threshold or NaN\n",
    "reset_condition = (oscillation_score < oscillation_threshold) | jnp.isnan(oscillation_score)\n",
    "\n",
    "# Identify currently silent interneurons (these will be permanently removed)\n",
    "silent_interneurons = interneuron_mask & (max_frs <= 0)\n",
    "\n",
    "key_next, subkey_continue, subkey_reset = random.split(key, 3)\n",
    "\n",
    "# === CONTINUE BRANCH: Normal pruning (oscillation is good) ===\n",
    "# Permanently remove silent interneurons\n",
    "total_removed_continue = total_removed_neurons | silent_interneurons\n",
    "\n",
    "# Update probabilities - exclude non-interneurons and removed neurons\n",
    "exclude_mask_continue = (~interneuron_mask) | total_removed_continue\n",
    "p_continue = removal_probability(max_frs, exclude_mask_continue)\n",
    "\n",
    "# Sample new neuron to remove (only from available interneurons)\n",
    "neuron_idx_continue = jax_choice(subkey_continue, jnp.arange(len(max_frs)), p_continue)\n",
    "\n",
    "# Update removed neurons\n",
    "removed_stim_continue = removed_stim_neurons.at[neuron_idx_continue].set(True)\n",
    "total_removed_continue = total_removed_continue.at[neuron_idx_continue].set(True)\n",
    "\n",
    "# Track what was removed this iteration (both silent and stimulated)\n",
    "newly_silent_continue = silent_interneurons & (~total_removed_neurons)  # Only newly silent\n",
    "last_removed_continue = jnp.full(last_removed.shape, False, dtype=jnp.bool_)\n",
    "last_removed_continue = last_removed_continue.at[neuron_idx_continue].set(True)  # Stimulated removal\n",
    "last_removed_continue = last_removed_continue | newly_silent_continue  # Add newly silent\n",
    "\n",
    "# Update other state\n",
    "level_continue = level + 1\n",
    "neurons_put_back_continue = neurons_put_back  # Unchanged\n",
    "min_circuit_continue = False  # Not converged yet\n",
    "\n",
    "# === RESET BRANCH: Restore last removed and try again ===\n",
    "# Restore ALL neurons from last_removed (both stimulated and those that went silent)\n",
    "# This includes neurons that went silent due to the last stimulated removal\n",
    "\n",
    "# Restore stimulated neurons from last removal\n",
    "removed_stim_reset = removed_stim_neurons & (~last_removed)\n",
    "\n",
    "# For total_removed: keep permanent removals from before last iteration, \n",
    "# add current silent neurons, but restore all last_removed neurons\n",
    "permanent_before_last = total_removed_neurons & (~last_removed)\n",
    "# Current silent neurons are those silent now (may include some that weren't silent before)\n",
    "# But we need to be careful not to restore neurons that are currently silent due to OTHER reasons\n",
    "# Only add neurons to total_removed if they are silent AND were not in last_removed\n",
    "currently_silent_not_restored = silent_interneurons & (~last_removed)\n",
    "total_removed_reset = permanent_before_last | currently_silent_not_restored\n",
    "\n",
    "# Track neurons being put back - ALL neurons from last_removed\n",
    "# This includes both the stimulated neuron and any neurons that went silent due to that removal\n",
    "restored_neurons = last_removed  # All neurons from last_removed are being restored\n",
    "neurons_put_back_reset = neurons_put_back | restored_neurons\n",
    "\n",
    "# Now select a different neuron to remove (avoid the restored ones)\n",
    "exclude_mask_reset = (~interneuron_mask) | total_removed_reset | restored_neurons\n",
    "p_reset = removal_probability(max_frs, exclude_mask_reset)\n",
    "\n",
    "# Check how many neurons are available\n",
    "available_neurons_reset = jnp.sum(interneuron_mask & (~exclude_mask_reset))\n",
    "\n",
    "# Select neuron to remove\n",
    "neuron_idx_reset = jax_choice(subkey_reset, jnp.arange(len(max_frs)), p_reset)\n",
    "\n",
    "# Only update if we have available neurons (otherwise keep current state)\n",
    "should_remove_new = available_neurons_reset > 0\n",
    "removed_stim_reset = jax.lax.select(\n",
    "    should_remove_new,\n",
    "    removed_stim_reset.at[neuron_idx_reset].set(True),\n",
    "    removed_stim_reset\n",
    ")\n",
    "total_removed_reset = jax.lax.select(\n",
    "    should_remove_new,\n",
    "    total_removed_reset.at[neuron_idx_reset].set(True),\n",
    "    total_removed_reset\n",
    ")\n",
    "\n",
    "# Track what was newly removed this iteration\n",
    "last_removed_reset = jnp.full(last_removed.shape, False, dtype=jnp.bool_)\n",
    "last_removed_reset = jax.lax.select(\n",
    "    should_remove_new,\n",
    "    last_removed_reset.at[neuron_idx_reset].set(True),\n",
    "    last_removed_reset\n",
    ")\n",
    "\n",
    "# Add any newly silent neurons (those that are silent now but weren't in total_removed_neurons before)\n",
    "# These are neurons that became silent due to current network state, not due to last removal\n",
    "newly_silent_reset = silent_interneurons & (~total_removed_neurons) & (~last_removed)\n",
    "last_removed_reset = last_removed_reset | newly_silent_reset\n",
    "\n",
    "# Keep level the same (we're trying again, not progressing)\n",
    "level_reset = level\n",
    "\n",
    "# Check if we've converged - either no more neurons to remove OR we're oscillating\n",
    "# Oscillation detection: if we're restoring neurons we've put back before, we're in a loop\n",
    "oscillation_detected = jnp.any(restored_neurons & neurons_put_back)\n",
    "min_circuit_reset = (available_neurons_reset <= 2) | oscillation_detected\n",
    "\n",
    "# === SELECT BETWEEN BRANCHES ===\n",
    "# Use jax.lax.select to choose between continue and reset results\n",
    "final_total_removed = jax.lax.select(reset_condition, total_removed_reset, total_removed_continue)\n",
    "final_removed_stim = jax.lax.select(reset_condition, removed_stim_reset, removed_stim_continue)\n",
    "final_last_removed = jax.lax.select(reset_condition, last_removed_reset, last_removed_continue)\n",
    "final_neurons_put_back = jax.lax.select(reset_condition, neurons_put_back_reset, neurons_put_back_continue)\n",
    "final_level = jax.lax.select(reset_condition, level_reset, level_continue)\n",
    "final_p = jax.lax.select(reset_condition, p_reset, p_continue)\n",
    "final_min_circuit = jax.lax.select(reset_condition, min_circuit_reset, min_circuit_continue)\n",
    "\n",
    "# Calculate available neurons and check for convergence\n",
    "available_neurons = jnp.sum(interneuron_mask & (~final_total_removed))\n",
    "\n",
    "# Check for oscillation: if we're in reset mode and detected oscillation, we've converged\n",
    "oscillation_converged = reset_condition & final_min_circuit\n",
    "size_converged = available_neurons <= 2\n",
    "final_converged = oscillation_converged | size_converged\n",
    "\n",
    "# Update W_mask to reflect removed neurons\n",
    "W_mask_init = jnp.ones_like(W_mask, dtype=jnp.float32)\n",
    "removed_float = final_total_removed.astype(jnp.float32)\n",
    "kept_mask = 1.0 - removed_float\n",
    "W_mask_new = W_mask_init * kept_mask[:, None] * kept_mask[None, :]\n",
    "\n",
    "# Convert to scalar for jax.lax.select\n",
    "final_converged_scalar = jnp.squeeze(final_converged)\n",
    "\n",
    "# Debug information\n",
    "jax.debug.print(\"Oscillation score: {score}\", score=oscillation_score)\n",
    "jax.debug.print(\"Reset condition (below threshold): {condition}\", condition=reset_condition)\n",
    "jax.debug.print(\"Available neurons: {count}\", count=available_neurons)\n",
    "jax.debug.print(\"Level: {level}\", level=final_level)\n",
    "jax.debug.print(\"Oscillation detected: {detected}\", detected=reset_condition & (final_min_circuit & ~size_converged))\n",
    "jax.debug.print(\"Final converged: {converged}\", converged=final_converged)\n",
    "jax.debug.print(\"Silent neurons removed: {count}\", count=jnp.sum(silent_interneurons))\n",
    "print('\\n')\n",
    "\n",
    "# When converged, preserve current state (don't make further changes)\n",
    "state = Pruning_state(\n",
    "    W_mask=jax.lax.select(final_converged_scalar, W_mask, W_mask_new),\n",
    "    interneuron_mask=interneuron_mask,\n",
    "    level=jax.lax.select(final_converged_scalar, level, final_level),\n",
    "    total_removed_neurons=jax.lax.select(final_converged_scalar, total_removed_neurons, final_total_removed),\n",
    "    neurons_put_back=jax.lax.select(final_converged_scalar, neurons_put_back, final_neurons_put_back),\n",
    "    removed_stim_neurons=jax.lax.select(final_converged_scalar, removed_stim_neurons, final_removed_stim),\n",
    "    last_removed=jax.lax.select(final_converged_scalar, last_removed, final_last_removed),\n",
    "    remove_p=jax.lax.select(final_converged_scalar, remove_p, final_p),\n",
    "    min_circuit=final_converged_scalar,\n",
    "    keys=key_next,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_inds(~load_state.total_removed_neurons[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_inds = print_inds(exclude_mask_continue)\n",
    "in_idxs.shape[0] - exclude_inds.shape[0]  # Number of interneurons left after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_inds(~exclude_mask_continue), print_inds(~exclude_mask_reset), print_inds(~final_total_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pytree(pytree):\n",
    "    \"\"\"\n",
    "    path_filter: function that takes key_path tuple and returns True/False\n",
    "    \"\"\"\n",
    "    def process_leaf(key_path, leaf):\n",
    "       print(f\"Key Path: {key_path[0]}, Leaf: {leaf.shape} dype: {leaf.dtype}\")\n",
    "\n",
    "    return jax.tree.map_with_path(process_leaf, pytree)\n",
    "print_pytree(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from src.sim_utils import compute_oscillation_score\n",
    "from src.optimized_vnc import update_params\n",
    "\n",
    "@dataclass\n",
    "class Pruning_state:\n",
    "    W_mask: jnp.ndarray\n",
    "    interneuron_mask: jnp.ndarray\n",
    "    level: int\n",
    "    total_removed_neurons: jnp.ndarray\n",
    "    neurons_put_back_current: jnp.ndarray\n",
    "    neurons_put_back_prev: jnp.ndarray\n",
    "    removed_stim_neurons: jnp.ndarray\n",
    "    last_removed: jnp.ndarray\n",
    "    remove_p: jnp.ndarray\n",
    "    round_complete: bool\n",
    "    converged: bool\n",
    "    round_number: int\n",
    "    keys: jax.random.PRNGKey\n",
    "    steps_in_current_round: int  # Track steps within current round\n",
    "    total_iterations: int        # Track total iterations for safety\n",
    "\n",
    "def safe_choice(key, logits, exclude_mask):\n",
    "    \"\"\"Safe choice function that works with vmap\"\"\"\n",
    "    # Set excluded neurons to very negative logits\n",
    "    safe_logits = jnp.where(exclude_mask, -1e10, logits)\n",
    "    # Use gumbel trick for sampling\n",
    "    gumbel_noise = jax.random.gumbel(key, safe_logits.shape)\n",
    "    return jnp.argmax(safe_logits + gumbel_noise)\n",
    "\n",
    "def removal_probability_safe(max_frs, exclude_mask):\n",
    "    \"\"\"Safe removal probability that works with vmap\"\"\"\n",
    "    # Create base probabilities (higher firing rate = lower removal probability)\n",
    "    base_probs = 1.0 / (max_frs + 1e-6)  # Add epsilon to avoid division by zero\n",
    "    # Zero out excluded neurons\n",
    "    probs = jnp.where(exclude_mask, 0.0, base_probs)\n",
    "    # Normalize (with safety for all-zero case)\n",
    "    total_prob = jnp.sum(probs)\n",
    "    return jnp.where(total_prob > 0, probs / total_prob, probs)\n",
    "\n",
    "def pruning_step_dynamic(state, R, mn_idxs, clip_start, oscillation_threshold):\n",
    "    \"\"\"\n",
    "    Single pruning step with dynamic convergence checking.\n",
    "    Works with vmap and continues until natural convergence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Safety check - stop if max iterations reached\n",
    "    max_iterations_reached = state.total_iterations >= 200\n",
    "    already_converged = state.converged\n",
    "    \n",
    "    # If we should stop, return unchanged state\n",
    "    should_stop = already_converged | max_iterations_reached\n",
    "    \n",
    "    def stopped_computation():\n",
    "        return state._replace(\n",
    "            converged=True,  # Mark as converged if we hit max iterations\n",
    "            total_iterations=state.total_iterations + 1\n",
    "        )\n",
    "    \n",
    "    def active_computation():\n",
    "        # Get neural activity\n",
    "        max_frs = jnp.max(R, axis=-1)\n",
    "        mn_mask = jnp.isin(jnp.arange(R.shape[0]), mn_idxs)\n",
    "        active_mask = (max_frs > 0) & mn_mask\n",
    "\n",
    "        # Compute oscillation score\n",
    "        oscillation_score, _ = compute_oscillation_score(R[..., clip_start:], active_mask, prominence=0.05)\n",
    "        reset_condition = (oscillation_score < oscillation_threshold) | jnp.isnan(oscillation_score)\n",
    "\n",
    "        # Identify silent interneurons (permanent removal)\n",
    "        silent_interneurons = state.interneuron_mask & (max_frs <= 0)\n",
    "        \n",
    "        # Generate key for this iteration\n",
    "        key = jax.random.fold_in(state.keys, state.total_iterations)\n",
    "        key_continue, key_reset = jax.random.split(key)\n",
    "        \n",
    "        # Check if we need to start a new round\n",
    "        def check_new_round():\n",
    "            \"\"\"Check if current round is complete and handle round transition\"\"\"\n",
    "            \n",
    "            # Current round is complete if no interneurons available for removal\n",
    "            exclude_mask = (~state.interneuron_mask) | state.total_removed_neurons | state.neurons_put_back_current\n",
    "            available_neurons = jnp.sum(state.interneuron_mask & (~exclude_mask))\n",
    "            current_round_done = available_neurons <= 0\n",
    "            \n",
    "            def start_new_round():\n",
    "                # Compare current and previous put-back lists for convergence\n",
    "                lists_identical = jnp.allclose(state.neurons_put_back_current, \n",
    "                                             state.neurons_put_back_prev, atol=1e-6)\n",
    "                \n",
    "                # If lists are identical, we've converged\n",
    "                new_converged = lists_identical\n",
    "                \n",
    "                # Start new round\n",
    "                return state._replace(\n",
    "                    neurons_put_back_prev=state.neurons_put_back_current,\n",
    "                    neurons_put_back_current=jnp.zeros_like(state.neurons_put_back_current),\n",
    "                    removed_stim_neurons=jnp.zeros_like(state.removed_stim_neurons),\n",
    "                    last_removed=jnp.zeros_like(state.last_removed),\n",
    "                    level=0,\n",
    "                    round_complete=False,\n",
    "                    converged=new_converged,\n",
    "                    round_number=state.round_number + 1,\n",
    "                    steps_in_current_round=0\n",
    "                )\n",
    "            \n",
    "            def continue_current_round():\n",
    "                return state\n",
    "            \n",
    "            # Start new round if current one is complete\n",
    "            return jax.lax.cond(current_round_done, start_new_round, continue_current_round)\n",
    "        \n",
    "        # Check for round transition first\n",
    "        state_after_round_check = check_new_round()\n",
    "        \n",
    "        # If we just converged in round transition, return that state\n",
    "        def handle_convergence():\n",
    "            return state_after_round_check._replace(\n",
    "                total_iterations=state.total_iterations + 1\n",
    "            )\n",
    "        \n",
    "        def normal_pruning_step():\n",
    "            \"\"\"Normal within-round pruning step\"\"\"\n",
    "            \n",
    "            # === CONTINUE BRANCH: Normal removal ===\n",
    "            total_removed_continue = state_after_round_check.total_removed_neurons | silent_interneurons\n",
    "            exclude_mask_continue = (~state_after_round_check.interneuron_mask) | total_removed_continue | state_after_round_check.neurons_put_back_current\n",
    "            \n",
    "            # Check if any neurons available for removal in continue branch\n",
    "            available_neurons_continue = jnp.sum(state_after_round_check.interneuron_mask & (~exclude_mask_continue))\n",
    "            \n",
    "            # Sample neuron to remove\n",
    "            p_continue = removal_probability_safe(max_frs, exclude_mask_continue)\n",
    "            neuron_idx_continue = safe_choice(key_continue, p_continue, exclude_mask_continue)\n",
    "            \n",
    "            # Update for continue branch (only if neurons available)\n",
    "            can_remove_continue = available_neurons_continue > 0\n",
    "            \n",
    "            removed_stim_continue = jnp.where(\n",
    "                can_remove_continue,\n",
    "                state_after_round_check.removed_stim_neurons.at[neuron_idx_continue].set(True),\n",
    "                state_after_round_check.removed_stim_neurons\n",
    "            )\n",
    "            total_removed_continue = jnp.where(\n",
    "                can_remove_continue,\n",
    "                total_removed_continue.at[neuron_idx_continue].set(True),\n",
    "                total_removed_continue\n",
    "            )\n",
    "            \n",
    "            # Track last removed\n",
    "            last_removed_continue = jnp.zeros_like(state_after_round_check.last_removed)\n",
    "            last_removed_continue = jnp.where(\n",
    "                can_remove_continue,\n",
    "                last_removed_continue.at[neuron_idx_continue].set(True),\n",
    "                last_removed_continue\n",
    "            )\n",
    "            # Add newly silent neurons\n",
    "            newly_silent = silent_interneurons & (~state_after_round_check.total_removed_neurons)\n",
    "            last_removed_continue = last_removed_continue | newly_silent\n",
    "            \n",
    "            # === RESET BRANCH: Restore and try different neuron ===\n",
    "            # Restore last removed neurons to put_back list\n",
    "            neurons_put_back_reset = state_after_round_check.neurons_put_back_current | state_after_round_check.last_removed\n",
    "            removed_stim_reset = state_after_round_check.removed_stim_neurons & (~state_after_round_check.last_removed)\n",
    "            total_removed_reset = (state_after_round_check.total_removed_neurons & (~state_after_round_check.last_removed)) | silent_interneurons\n",
    "            \n",
    "            exclude_mask_reset = (~state_after_round_check.interneuron_mask) | total_removed_reset | neurons_put_back_reset\n",
    "            available_neurons_reset = jnp.sum(state_after_round_check.interneuron_mask & (~exclude_mask_reset))\n",
    "            \n",
    "            # Sample different neuron\n",
    "            p_reset = removal_probability_safe(max_frs, exclude_mask_reset)\n",
    "            neuron_idx_reset = safe_choice(key_reset, p_reset, exclude_mask_reset)\n",
    "            \n",
    "            can_remove_reset = available_neurons_reset > 0\n",
    "            \n",
    "            removed_stim_reset = jnp.where(\n",
    "                can_remove_reset,\n",
    "                removed_stim_reset.at[neuron_idx_reset].set(True),\n",
    "                removed_stim_reset\n",
    "            )\n",
    "            total_removed_reset = jnp.where(\n",
    "                can_remove_reset,\n",
    "                total_removed_reset.at[neuron_idx_reset].set(True),\n",
    "                total_removed_reset\n",
    "            )\n",
    "            \n",
    "            last_removed_reset = jnp.zeros_like(state_after_round_check.last_removed)\n",
    "            last_removed_reset = jnp.where(\n",
    "                can_remove_reset,\n",
    "                last_removed_reset.at[neuron_idx_reset].set(True),\n",
    "                last_removed_reset\n",
    "            )\n",
    "            # Add newly silent neurons (excluding those already in last_removed)\n",
    "            newly_silent_reset = silent_interneurons & (~state_after_round_check.total_removed_neurons) & (~state_after_round_check.last_removed)\n",
    "            last_removed_reset = last_removed_reset | newly_silent_reset\n",
    "            \n",
    "            # Choose between branches based on reset condition\n",
    "            final_total_removed = jnp.where(reset_condition, total_removed_reset, total_removed_continue)\n",
    "            final_removed_stim = jnp.where(reset_condition, removed_stim_reset, removed_stim_continue)\n",
    "            final_last_removed = jnp.where(reset_condition, last_removed_reset, last_removed_continue)\n",
    "            final_neurons_put_back = jnp.where(reset_condition, neurons_put_back_reset, state_after_round_check.neurons_put_back_current)\n",
    "            final_remove_p = jnp.where(reset_condition, p_reset, p_continue)\n",
    "            \n",
    "            # Update W_mask based on removed neurons\n",
    "            removed_float = final_total_removed.astype(jnp.float32)\n",
    "            kept_mask = 1.0 - removed_float\n",
    "            W_mask_new = kept_mask[:, None] * kept_mask[None, :]\n",
    "            \n",
    "            return state_after_round_check._replace(\n",
    "                W_mask=W_mask_new,\n",
    "                level=state_after_round_check.level + 1,\n",
    "                total_removed_neurons=final_total_removed,\n",
    "                neurons_put_back_current=final_neurons_put_back,\n",
    "                removed_stim_neurons=final_removed_stim,\n",
    "                last_removed=final_last_removed,\n",
    "                remove_p=final_remove_p,\n",
    "                steps_in_current_round=state_after_round_check.steps_in_current_round + 1,\n",
    "                total_iterations=state.total_iterations + 1\n",
    "            )\n",
    "        \n",
    "        # Choose between convergence handling and normal step\n",
    "        return jax.lax.cond(\n",
    "            state_after_round_check.converged, \n",
    "            handle_convergence, \n",
    "            normal_pruning_step\n",
    "        )\n",
    "    \n",
    "    # Main conditional: stop or continue\n",
    "    return jax.lax.cond(should_stop, stopped_computation, active_computation)\n",
    "\n",
    "# Vmap-compatible version\n",
    "@partial(jax.vmap, in_axes=(0, 0, None, None, None))\n",
    "def pruning_step_batched(state_batch, R_batch, mn_idxs, clip_start, oscillation_threshold):\n",
    "    \"\"\"Vectorized pruning step that works across a batch\"\"\"\n",
    "    return pruning_step_dynamic(state_batch, R_batch, mn_idxs, clip_start, oscillation_threshold)\n",
    "\n",
    "def run_pruning_algorithm_batched(neuron_params, sim_params, initial_states, Rs, mn_idxs, clip_start, oscillation_threshold):\n",
    "    \"\"\"\n",
    "    Run pruning algorithm on batch until convergence or max iterations (200).\n",
    "    \n",
    "    Args:\n",
    "        initial_states: Batched Pruning_state \n",
    "        Rs: Batch of neural activity [batch_size, neurons, time]\n",
    "        mn_idxs: Motor neuron indices\n",
    "        clip_start: Start index for oscillation analysis\n",
    "        oscillation_threshold: Threshold for oscillation score\n",
    "    \n",
    "    Returns:\n",
    "        final_states: Final states for each batch element\n",
    "        iterations_used: Number of iterations each element used\n",
    "    \"\"\"\n",
    "    \n",
    "    def should_continue(state_batch):\n",
    "        \"\"\"Check if any element in batch needs to continue\"\"\"\n",
    "        return jnp.any(~state_batch.converged)\n",
    "    \n",
    "    def iteration_step(state_batch):\n",
    "        \"\"\"Single iteration across the batch\"\"\"\n",
    "        neuron_params = neuron_params._replace(W_mask=state.W_mask)\n",
    "        batch_results = batch_func(neuron_params, sim_params, batch_indices)\n",
    "        return pruning_step_batched(state_batch, Rs, mn_idxs, clip_start, oscillation_threshold)\n",
    "    \n",
    "    # Run until all converged or max iterations reached\n",
    "    current_states = initial_states\n",
    "    max_iterations = 200\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        # Take one step for all batch elements\n",
    "        current_states = iteration_step(current_states)\n",
    "        \n",
    "        # Check if all converged - optional early stopping for efficiency\n",
    "        if not should_continue(current_states):\n",
    "            break\n",
    "    \n",
    "    return current_states\n",
    "\n",
    "def create_batched_initial_state(batch_size, n_neurons, key=None):\n",
    "    \"\"\"Helper to create batched initial states\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    keys = jax.random.split(key, batch_size)\n",
    "    \n",
    "    return Pruning_state(\n",
    "        W_mask=jnp.ones((batch_size, n_neurons, n_neurons)),\n",
    "        interneuron_mask=jnp.ones((batch_size, n_neurons), dtype=bool),\n",
    "        level=jnp.zeros(batch_size, dtype=int),\n",
    "        total_removed_neurons=jnp.zeros((batch_size, n_neurons), dtype=bool),\n",
    "        neurons_put_back_current=jnp.zeros((batch_size, n_neurons), dtype=bool),\n",
    "        neurons_put_back_prev=jnp.zeros((batch_size, n_neurons), dtype=bool),\n",
    "        removed_stim_neurons=jnp.zeros((batch_size, n_neurons), dtype=bool),\n",
    "        last_removed=jnp.zeros((batch_size, n_neurons), dtype=bool),\n",
    "        remove_p=jnp.ones((batch_size, n_neurons)) / n_neurons,\n",
    "        round_complete=jnp.zeros(batch_size, dtype=bool),\n",
    "        converged=jnp.zeros(batch_size, dtype=bool),\n",
    "        round_number=jnp.zeros(batch_size, dtype=int),\n",
    "        keys=keys,\n",
    "        steps_in_current_round=jnp.zeros(batch_size, dtype=int),\n",
    "        total_iterations=jnp.zeros(batch_size, dtype=int)\n",
    "    )\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Create batch\n",
    "batch_size = 16\n",
    "n_neurons = 100\n",
    "initial_states = create_batched_initial_state(batch_size, n_neurons)\n",
    "\n",
    "# Your neural activity data\n",
    "Rs = jnp.ones((batch_size, n_neurons, 1000))  # [batch, neurons, time]\n",
    "mn_idxs = jnp.array([0, 1, 2])  # Motor neuron indices\n",
    "\n",
    "# Run algorithm\n",
    "final_states = run_pruning_algorithm_batched(\n",
    "    neuron_params, sim_params,\n",
    "    initial_states, Rs, mn_idxs, \n",
    "    clip_start=100, oscillation_threshold=0.5\n",
    ")\n",
    "\n",
    "# Check results\n",
    "print(f\"Converged: {jnp.sum(final_states.converged)} / {batch_size}\")\n",
    "print(f\"Iterations used: {final_states.total_iterations}\")\n",
    "print(f\"Hit max iterations: {jnp.sum(final_states.total_iterations >= 200)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparse.load_npz('/data/users/eabe/Pugliese_2025/Prune_Test/debug/run_id=Testing/sim=prune_network/ckpt/Prune_Test_Rs.npz').todense().astype(jnp.float32).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sparse.load_npz(cfg.paths.ckpt_dir  / f\"{cfg.experiment.name}_Rs.npz\").todense().astype(np.float32).squeeze()\n",
    "mini_circuit = sparse.load_npz(cfg.paths.ckpt_dir  / f\"{cfg.experiment.name}_mini_circuits.npz\").todense().astype(np.bool).squeeze()\n",
    "# W_mask = sparse.load_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_W_mask.npz\").todense().astype(np.float32)\n",
    "# total_removed_neurons = sparse.load_npz(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_total_removed.npz\").todense().astype(np.bool_)\n",
    "# loaded_state = load_state(cfg.paths.ckpt_dir / f\"{cfg.experiment.name}_state.pkl\")\n",
    "# loaded_state = load_state('/data/users/eabe/Pugliese_2025/Prune_Test/debug/run_id=Testing/sim=prune_network/ckpt/Prune_Test_state.pkl')\n",
    "w_table = load_wTable(cfg.experiment.dfPath)\n",
    "n_stim_configs = len(cfg.experiment.stimNeurons)\n",
    "## Initialize parameters\n",
    "neuron_params = prepare_neuron_params(cfg, w_table)\n",
    "sim_params = prepare_sim_params(cfg, n_stim_configs, neuron_params.W.shape[0])\n",
    "all_neurons = w_table.index.to_numpy()\n",
    "n_sims = sim_params.n_param_sets * sim_params.n_stim_configs\n",
    "mn_idxs = jnp.asarray(w_table.loc[w_table[\"class\"]==\"motor neuron\"].index.values)\n",
    "in_idxs = jnp.setdiff1d(all_neurons, mn_idxs)\n",
    "nonmn_idxs = w_table.loc[(w_table[\"bodyId\"]==10093) | (w_table[\"bodyId\"]==10707) | (w_table[\"bodyId\"]==13905) | (w_table[\"bodyId\"]==11751)].index.values\n",
    "interneuron_mask = jnp.full((neuron_params.W_mask.shape[0], neuron_params.W_mask.shape[-1]), fill_value=False, dtype=jnp.bool_)\n",
    "interneuron_mask = interneuron_mask.at[:, in_idxs].set(True)\n",
    "# neuron_params = neuron_params._replace(W_mask=loaded_state.W_mask)\n",
    "# mini_circuit = ((~loaded_state.total_removed_neurons) | loaded_state.last_removed | loaded_state.prev_put_back)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_params.W_mask.shape, mini_circuit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = results\n",
    "# mini_circuit = ((~loaded_state.total_removed_neurons) | loaded_state.last_removed | loaded_state.prev_put_back)\n",
    "W_mask_init = jnp.ones_like(neuron_params.W_mask, dtype=jnp.bool)\n",
    "W_mask_new = W_mask_init * mini_circuit[:, :, None] * mini_circuit[:, None, :] \n",
    "neuron_params = neuron_params._replace(W_mask=(W_mask_new))\n",
    "# results = process_batch_prune(neuron_params, sim_params, jnp.arange(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.squeeze()\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R = batch_results[0]\n",
    "n_replicate = 510\n",
    "R = results[n_replicate]\n",
    "interneuron_left = jnp.where(mini_circuit[n_replicate]&(interneuron_mask[n_replicate]))[0]\n",
    "print(f\"interneurons left: {interneuron_left}\")\n",
    "print(f\"prev_min_circuit neurons: {nonmn_idxs}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "ax = axs[0]\n",
    "ax.plot(R[nonmn_idxs].T)\n",
    "ax.set_title(\"mini_circuit neurons\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.plot(R[mn_idxs].T)\n",
    "ax.set_title(\"motor neurons\")\n",
    "\n",
    "ax = axs[2]\n",
    "active_neurons = jnp.where((jnp.sum(results[0],axis=-1) > 0))[0]\n",
    "ax.plot(R[active_neurons].T)\n",
    "ax.set_title(\"active neurons\")\n",
    "\n",
    "# Get active MN activity using JAX-compatible approach\n",
    "max_frs = jnp.max(R, axis=-1)\n",
    "mn_mask = jnp.isin(jnp.arange(R.shape[0]), mn_idxs)\n",
    "active_mask = ((max_frs>0) & mn_mask)\n",
    "clip_start = 220\n",
    "# Compute oscillation score\n",
    "oscillation_score, _ = compute_oscillation_score(R[..., clip_start:], active_mask, prominence=0.05)\n",
    "# print('oscillation_score:', oscillation_score)\n",
    "oscillation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_interneurons = []\n",
    "active_motor_neurons = []\n",
    "for n in range(results.shape[0]):\n",
    "    R = results[n]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    active_interneurons.append(jnp.where((max_frs > 0) & interneuron_mask[n])[0])\n",
    "    active_motor_neurons.append(jnp.where((max_frs > 0) & ~interneuron_mask[n])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_interneurons\n",
    "active_motor_neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sim_utils import neuron_oscillation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oscillation_score, _ = compute_oscillation_score(results.reshape(-1, results.shape[-1])[..., clip_start:], active_mask, prominence=0.05)\n",
    "activity = results[..., clip_start:]\n",
    "prominence = 0.05\n",
    "all_scores = []\n",
    "for nsim in range(activity.shape[0]):\n",
    "    R = activity[nsim]\n",
    "    max_frs = jnp.max(R, axis=-1)\n",
    "    mn_mask = jnp.isin(jnp.arange(R.shape[0]), mn_idxs)\n",
    "    active_mask = ((max_frs>0) & mn_mask)\n",
    "    sim_scores, freqs = jax.vmap(neuron_oscillation_score, in_axes=(0, None))(R, prominence)\n",
    "    all_scores.append(sim_scores[active_mask])\n",
    "all_scores = jnp.concatenate(all_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_scores, bins=50, density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stim Adjustment Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_with_stim_adjustment(self,maxIters=10,clampedNeurons=[],clampedRates=None,nActiveUpper=500,nActiveLower=5,nHighFrUpper=100):\n",
    "    nextHighest = None\n",
    "    nextLowest = None\n",
    "\n",
    "    for i in range(maxIters):\n",
    "        self.run(clampedNeurons=clampedNeurons,clampedRates=clampedRates)\n",
    "        R = self.R\n",
    "\n",
    "        nActive = sum(np.sum(R,1)>0)\n",
    "        nHighFr = sum(np.max(R,1)>100)\n",
    "\n",
    "        currInputs = self.inputs.copy()\n",
    "\n",
    "        print(f\"Run {i}\")\n",
    "        print(f\"max stimI = {np.max(currInputs)}\")\n",
    "        print(f\"nActive: {nActive}\")\n",
    "        print(f\"nHighFr: {nHighFr}\")\n",
    "\n",
    "        if (nActive > nActiveUpper) or (nHighFr > nHighFrUpper): # too strong\n",
    "            if nextLowest is None:\n",
    "                newInputs = currInputs/2\n",
    "            else:\n",
    "                newInputs = (currInputs+nextLowest)/2\n",
    "            nextHighest = currInputs\n",
    "        elif (nActive < nActiveLower): # too weak\n",
    "            if nextHighest is None:\n",
    "                newInputs = currInputs*2\n",
    "            else:\n",
    "                newInputs = (currInputs+nextHighest)/2\n",
    "            nextLowest = currInputs\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        self.set_input(newInputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note: The run_with_stim_adjustment function cannot be easily JIT-compiled\n",
    "# because it involves loops with data-dependent control flow and side effects.\n",
    "# Here's a restructured version that separates the JIT-able parts:\n",
    "\n",
    "@jit\n",
    "def compute_activity_metrics(R):\n",
    "    \"\"\"JIT-compatible function to compute activity metrics.\"\"\"\n",
    "    n_active = jnp.sum(jnp.sum(R, axis=1) > 0)\n",
    "    n_high_fr = jnp.sum(jnp.max(R, axis=1) > 100)\n",
    "    return n_active, n_high_fr\n",
    "\n",
    "@jit\n",
    "def update_inputs_binary_search(curr_inputs, next_lowest, next_highest, \n",
    "                               n_active, n_high_fr, n_active_upper, \n",
    "                               n_active_lower, n_high_fr_upper):\n",
    "    \"\"\"JIT-compatible input update logic.\"\"\"\n",
    "    \n",
    "    # Determine if stimulation is too strong\n",
    "    too_strong = (n_active > n_active_upper) | (n_high_fr > n_high_fr_upper)\n",
    "    too_weak = n_active < n_active_lower\n",
    "    \n",
    "    # Update inputs based on binary search logic\n",
    "    def update_for_too_strong():\n",
    "        new_inputs = jnp.where(\n",
    "            next_lowest is None,\n",
    "            curr_inputs / 2,\n",
    "            (curr_inputs + next_lowest) / 2\n",
    "        )\n",
    "        new_next_highest = curr_inputs\n",
    "        return new_inputs, next_lowest, new_next_highest\n",
    "    \n",
    "    def update_for_too_weak():\n",
    "        new_inputs = jnp.where(\n",
    "            next_highest is None,\n",
    "            curr_inputs * 2,\n",
    "            (curr_inputs + next_highest) / 2\n",
    "        )\n",
    "        new_next_lowest = curr_inputs\n",
    "        return new_inputs, new_next_lowest, next_highest\n",
    "    \n",
    "    def no_update():\n",
    "        return curr_inputs, next_lowest, next_highest\n",
    "    \n",
    "    # Apply updates conditionally\n",
    "    new_inputs, new_next_lowest, new_next_highest = jax.lax.cond(\n",
    "        too_strong,\n",
    "        update_for_too_strong,\n",
    "        lambda: jax.lax.cond(\n",
    "            too_weak,\n",
    "            update_for_too_weak,\n",
    "            no_update\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    converged = ~too_strong & ~too_weak\n",
    "    \n",
    "    return new_inputs, new_next_lowest, new_next_highest, converged\n",
    "\n",
    "# Example usage:\n",
    "def run_with_stim_adjustment_jax(simulation_runner, max_iters=10, \n",
    "                                clamped_neurons=None, clamped_rates=None,\n",
    "                                n_active_upper=500, n_active_lower=5, \n",
    "                                n_high_fr_upper=100):\n",
    "    \"\"\"\n",
    "    JAX-compatible version of run_with_stim_adjustment.\n",
    "    Note: This requires the simulation_runner to be compatible with JAX.\n",
    "    \"\"\"\n",
    "    next_highest = None\n",
    "    next_lowest = None\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        # Run simulation (this part depends on your simulation framework)\n",
    "        R = simulation_runner.run(clamped_neurons=clamped_neurons, \n",
    "                                 clamped_rates=clamped_rates)\n",
    "        \n",
    "        # Compute metrics (JIT-compiled)\n",
    "        n_active, n_high_fr = compute_activity_metrics(R)\n",
    "        curr_inputs = simulation_runner.get_inputs()\n",
    "        \n",
    "        print(f\"Run {i}\")\n",
    "        print(f\"max stimI = {jnp.max(curr_inputs)}\")\n",
    "        print(f\"nActive: {n_active}\")\n",
    "        print(f\"nHighFr: {n_high_fr}\")\n",
    "        \n",
    "        # Update inputs (JIT-compiled)\n",
    "        new_inputs, next_lowest, next_highest, converged = update_inputs_binary_search(\n",
    "            curr_inputs, next_lowest, next_highest, n_active, n_high_fr,\n",
    "            n_active_upper, n_active_lower, n_high_fr_upper\n",
    "        )\n",
    "        \n",
    "        if converged:\n",
    "            break\n",
    "            \n",
    "        simulation_runner.set_input(new_inputs)\n",
    "\n",
    "# Additional utility functions for JAX compatibility:\n",
    "\n",
    "@jit\n",
    "def safe_divide(x, y, default=0.0):\n",
    "    \"\"\"Safe division that handles division by zero.\"\"\"\n",
    "    return jnp.where(y == 0, default, x / y)\n",
    "\n",
    "@jit\n",
    "def safe_max(x, default=0.0):\n",
    "    \"\"\"Safe max that handles empty arrays.\"\"\"\n",
    "    return jnp.where(len(x) == 0, default, jnp.max(x))\n",
    "\n",
    "@jit\n",
    "def safe_min(x, default=0.0):\n",
    "    \"\"\"Safe min that handles empty arrays.\"\"\"\n",
    "    return jnp.where(len(x) == 0, default, jnp.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnc-closedloop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
